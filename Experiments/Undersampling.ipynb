{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Undersampling Experiments\n",
        "\n",
        "This notebook runs experiments to evaluate uncertainty quantification methods when training data is non-uniformly distributed (spatially undersampled).\n",
        "\n",
        "The experiments train models on data with different sampling densities across regions (e.g., well-sampled in the middle, undersampled on the sides) and evaluate uncertainties across these regions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from typing import Tuple\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pyro\n",
        "\n",
        "# Add parent directory to path to import Models\n",
        "# This works for notebooks in the Experiments folder\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'Experiments' else Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Setup results directory\n",
        "results_dir = project_root / \"results\" / \"undersampling\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "plots_dir = results_dir / \"plots\"\n",
        "plots_dir.mkdir(exist_ok=True)\n",
        "stats_dir = results_dir / \"statistics\"\n",
        "stats_dir.mkdir(exist_ok=True)\n",
        "outputs_dir = results_dir / \"outputs\"\n",
        "outputs_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Results will be saved to: {results_dir}\")\n",
        "\n",
        "# Import from Models folder\n",
        "from Models.MC_Dropout import (\n",
        "    MCDropoutRegressor,\n",
        "    train_model,\n",
        "    mc_dropout_predict,\n",
        "    gaussian_nll,\n",
        "    beta_nll,\n",
        "    plot_toy_data,\n",
        "    plot_uncertainties,\n",
        "    normalize_x,\n",
        "    normalize_x_data\n",
        ")\n",
        "\n",
        "from Models.Deep_Ensemble import (\n",
        "    train_ensemble_deep,\n",
        "    ensemble_predict_deep\n",
        ")\n",
        "\n",
        "from Models.BNN import (\n",
        "    train_bnn,\n",
        "    bnn_predict,\n",
        "    normalize_x as bnn_normalize_x,\n",
        "    normalize_x_data as bnn_normalize_x_data\n",
        ")\n",
        "\n",
        "from Models.BAMLSS import (\n",
        "    fit_bamlss,\n",
        "    bamlss_predict\n",
        ")\n",
        "\n",
        "from utils.device import get_device\n",
        "from utils.plotting import plot_uncertainties_undersampling\n",
        "import utils.results_save as results_save_module\n",
        "from utils.results_save import save_plot, save_statistics, save_summary_text, save_summary_statistics\n",
        "from utils.plotting import plot_data_with_ood_regions\n",
        "\n",
        "# Import helper functions for undersampling experiments\n",
        "from utils.undersampling_experiments import (\n",
        "    run_mc_dropout_undersampling_experiment,\n",
        "    run_deep_ensemble_undersampling_experiment,\n",
        "    run_bnn_undersampling_experiment,\n",
        "    run_bamlss_undersampling_experiment\n",
        ")\n",
        "\n",
        "# Set the module-level directories for results_save\n",
        "results_save_module.plots_dir = plots_dir\n",
        "results_save_module.stats_dir = stats_dir\n",
        "results_save_module.outputs_dir = outputs_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Device Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = get_device()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Toy Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# ----- Data generation for linear function with homo/heteroscedastic noise -----\n",
        "# f(x) = 0.7x + 0.5\n",
        "# noise_type: 'homoscedastic' (σ(x) = 0.20) or 'heteroscedastic' (σ(x) = 0.10 + 0.2(0.5 + 0.5sin(x)))\n",
        "def generate_toy_regression(n_train=1000, train_range=(-5, 10.0), grid_points=1000, noise_type='heteroscedastic', type = \"linear\"):\n",
        "    \"\"\"\n",
        "    Generate toy regression data.\n",
        "    \n",
        "    Args:\n",
        "        n_train: Number of training samples\n",
        "        train_range: Training range tuple (min, max)\n",
        "        grid_points: Number of grid points for evaluation\n",
        "        noise_type: 'homoscedastic' or 'heteroscedastic'\n",
        "        type: 'linear' or 'sin'\n",
        "    \n",
        "    Returns:\n",
        "        (x_train, y_train, x_grid, y_grid_clean)\n",
        "    \"\"\"\n",
        "    # Sample training data uniformly from train_range\n",
        "    x_train = np.random.uniform(train_range[0], train_range[1], size=(n_train, 1))\n",
        "    \n",
        "    if type == \"linear\":\n",
        "        # Linear function: f(x) = 0.7x + 0.5\n",
        "        f_clean = lambda x: 0.7 * x + 0.5\n",
        "        y_clean_train = f_clean(x_train)\n",
        "    elif type == \"sin\":\n",
        "        f_clean = lambda x:  x * np.sin(x) + x\n",
        "        y_clean_train = f_clean(x_train)\n",
        "    else:\n",
        "        raise ValueError(\"type must be 'linear', 'sin'\")\n",
        "\n",
        "    # Define noise variance σ²(x)\n",
        "    if noise_type == 'homoscedastic':\n",
        "        # Homoscedastic: σ(x) = 2\n",
        "        sigma = 2\n",
        "        sigma_train = np.full_like(x_train, sigma)\n",
        "    elif noise_type == 'heteroscedastic':\n",
        "        # Heteroscedastic: \n",
        "        sigma_train = np.abs(2.5 * np.sin(0.5*x_train +5))\n",
        "    else:\n",
        "        raise ValueError(\"noise_type must be 'homoscedastic' or 'heteroscedastic'\")\n",
        "    \n",
        "    # Generate noise: ε | x ~ N(0, σ²(x))\n",
        "    epsilon = np.random.normal(0.0, sigma_train, size=(n_train, 1))\n",
        "    y_train = y_clean_train + epsilon\n",
        "\n",
        "    # Dense evaluation grid\n",
        "    x_grid = np.linspace(train_range[0], train_range[1], grid_points).reshape(-1, 1)\n",
        "    y_grid_clean = f_clean(x_grid)\n",
        "\n",
        "    return (x_train.astype(np.float32), y_train.astype(np.float32),\n",
        "            x_grid.astype(np.float32), y_grid_clean.astype(np.float32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_train = 1000\n",
        "train_range = (-5, 10)\n",
        "# Define sampling regions with different densities\n",
        "# Format: [(region_tuple, density_factor), ...]\n",
        "# density_factor: 0.0-1.0+ representing relative sampling density\n",
        "# Example: [((-5, 0), 0.2), ((0, 5), 1.0), ((5, 10), 0.2)]\n",
        "#   - Regions (-5, 0) and (5, 10) are undersampled (density 0.2)\n",
        "#   - Region (0, 5) is well-sampled (density 1.0)\n",
        "sampling_regions = [((-5, 0), 1), ((0, 5), 0.2), ((5, 10), 1)]\n",
        "grid_points = 1500\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MC Dropout - Homoscedastic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_mc_dropout_undersampling_experiment(\n",
        "    generate_toy_regression_func=generate_toy_regression,\n",
        "    function_types=['linear', 'sin'],\n",
        "    noise_type='homoscedastic',\n",
        "    train_range=train_range,\n",
        "    sampling_regions=sampling_regions,\n",
        "    n_train=n_train,\n",
        "    grid_points=grid_points,\n",
        "    seed=seed,\n",
        "    p=0.2,\n",
        "    beta=0.5,\n",
        "    epochs=700,\n",
        "    lr=1e-3,\n",
        "    batch_size=32,\n",
        "    mc_samples=20,\n",
        "    entropy_method='analytical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from Models.BNN import decompose_uncertainty\n",
        "from utils.entropy_uncertainty import entropy_uncertainty_analytical, entropy_uncertainty_numerical\n",
        "from utils.plotting import plot_uncertainties_undersampling, plot_uncertainties_entropy_undersampling\n",
        "\n",
        "# 1. Load the saved outputs\n",
        "filepath = Path(r\"C:\\Users\\lukas\\OneDrive\\Desktop\\Code-Masterarbeit\\A-statistical-evaluation-of-uncertainty-disentanglement-methods-1\\results\\undersampling\\outputs\\undersampling\\homoscedastic\\sin\\20260103_MC_Dropout_p0.2_M20_raw_outputs.npz\")\n",
        "data = np.load(filepath, allow_pickle=True)\n",
        "\n",
        "# 2. Extract arrays\n",
        "mu_samples = data['mu_samples']  # Shape: [M, N] for MC Dropout, [K, N] for Deep Ensemble, [S, N] for BNN/BAMLSS\n",
        "sigma2_samples = data['sigma2_samples']  # Same shape\n",
        "x_grid = data['x_grid']\n",
        "y_grid_clean = data['y_grid_clean']\n",
        "\n",
        "# Extract metadata (they're stored as arrays, so use [0] to get the value)\n",
        "model_name = str(data['model_name'][0]) if 'model_name' in data else None\n",
        "noise_type = str(data['noise_type'][0]) if 'noise_type' in data else None\n",
        "func_type = str(data['func_type'][0]) if 'func_type' in data else None\n",
        "\n",
        "# Optional: Extract training data if saved\n",
        "x_train = data['x_train_subset'] if 'x_train_subset' in data else None\n",
        "y_train = data['y_train_subset'] if 'y_train_subset' in data else None\n",
        "\n",
        "# 3. Recompute variance-based uncertainties\n",
        "# Use decompose_uncertainty from Models.BNN (works for all models)\n",
        "mu_pred, ale_var, epi_var, tot_var = decompose_uncertainty(mu_samples, np.sqrt(sigma2_samples))\n",
        "\n",
        "# 4. Recompute entropy-based uncertainties\n",
        "entropy_results = entropy_uncertainty_analytical(mu_samples, sigma2_samples)\n",
        "ale_entropy = entropy_results['aleatoric']\n",
        "epi_entropy = entropy_results['epistemic']\n",
        "tot_entropy = entropy_results['total']\n",
        "\n",
        "# 5. Create plots\n",
        "# For variance-based uncertainties (if you have region_masks and sampling_regions):\n",
        "if x_train is not None and y_train is not None:\n",
        "    # You'll need to define region_masks and sampling_regions based on your experiment\n",
        "    # For example, if you know the regions:\n",
        "    sampling_regions = [((-5, 0), 1), ((0, 5), 0.2), ((5, 10), 1)]\n",
        "    region_masks = []\n",
        "    for region_tuple, _ in sampling_regions:\n",
        "        mask = (x_grid[:, 0] >= region_tuple[0]) & (x_grid[:, 0] <= region_tuple[1])\n",
        "        region_masks.append(mask)\n",
        "    \n",
        "    # Plot variance-based uncertainties\n",
        "    plot_uncertainties_undersampling(\n",
        "        x_train, y_train, x_grid, y_grid_clean,\n",
        "        mu_pred, ale_var, epi_var, tot_var, \n",
        "        region_masks, sampling_regions,\n",
        "        title=f\"{model_name} - Recomputed - Variance\",\n",
        "        noise_type=noise_type,\n",
        "        func_type=func_type\n",
        "    )\n",
        "    \n",
        "    # Plot entropy-based uncertainties\n",
        "    plot_uncertainties_entropy_undersampling(\n",
        "        x_train, y_train, x_grid, y_grid_clean,\n",
        "        mu_pred, ale_entropy, epi_entropy, tot_entropy,\n",
        "        region_masks, sampling_regions,\n",
        "        title=f\"{model_name} - Recomputed - Entropy\",\n",
        "        noise_type=noise_type,\n",
        "        func_type=func_type\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MC Dropout - Heteroscedastic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_mc_dropout_undersampling_experiment(\n",
        "    generate_toy_regression_func=generate_toy_regression,\n",
        "    function_types=['linear', 'sin'],\n",
        "    noise_type='heteroscedastic',\n",
        "    train_range=train_range,\n",
        "    sampling_regions=sampling_regions,\n",
        "    n_train=n_train,\n",
        "    grid_points=grid_points,\n",
        "    seed=seed,\n",
        "    p=0.2,\n",
        "    beta=0.5,\n",
        "    epochs=700,\n",
        "    lr=1e-3,\n",
        "    batch_size=32,\n",
        "    mc_samples=20,\n",
        "    entropy_method='analytical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Ensemble - Homoscedastic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_deep_ensemble_undersampling_experiment(\n",
        "    generate_toy_regression_func=generate_toy_regression,\n",
        "    function_types=['linear', 'sin'],\n",
        "    noise_type='homoscedastic',\n",
        "    train_range=train_range,\n",
        "    sampling_regions=sampling_regions,\n",
        "    n_train=n_train,\n",
        "    grid_points=grid_points,\n",
        "    seed=seed,\n",
        "    beta=0.5,\n",
        "    batch_size=32,\n",
        "    K=5,\n",
        "    entropy_method='analytical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Ensemble - Heteroscedastic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_deep_ensemble_undersampling_experiment(\n",
        "    generate_toy_regression_func=generate_toy_regression,\n",
        "    function_types=['linear', 'sin'],\n",
        "    noise_type='heteroscedastic',\n",
        "    train_range=train_range,\n",
        "    sampling_regions=sampling_regions,\n",
        "    n_train=n_train,\n",
        "    grid_points=grid_points,\n",
        "    seed=seed,\n",
        "    beta=0.5,\n",
        "    batch_size=32,\n",
        "    K=5,\n",
        "    entropy_method='analytical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BNN - Homoscedastic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_bnn_undersampling_experiment(\n",
        "    generate_toy_regression_func=generate_toy_regression,\n",
        "    function_types=['linear', 'sin'],\n",
        "    noise_type='homoscedastic',\n",
        "    train_range=train_range,\n",
        "    sampling_regions=sampling_regions,\n",
        "    n_train=n_train,\n",
        "    grid_points=grid_points,\n",
        "    seed=seed,\n",
        "    hidden_width=16,\n",
        "    weight_scale=1.0,\n",
        "    warmup=200,\n",
        "    samples=200,\n",
        "    chains=1,\n",
        "    entropy_method='analytical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BNN - Heteroscedastic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_bnn_undersampling_experiment(\n",
        "    generate_toy_regression_func=generate_toy_regression,\n",
        "    function_types=['linear', 'sin'],\n",
        "    noise_type='heteroscedastic',\n",
        "    train_range=train_range,\n",
        "    sampling_regions=sampling_regions,\n",
        "    n_train=n_train,\n",
        "    grid_points=grid_points,\n",
        "    seed=seed,\n",
        "    hidden_width=16,\n",
        "    weight_scale=1.0,\n",
        "    warmup=200,\n",
        "    samples=200,\n",
        "    chains=1,\n",
        "    entropy_method='analytical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BAMLSS - Homoscedastic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_bamlss_undersampling_experiment(\n",
        "    generate_toy_regression_func=generate_toy_regression,\n",
        "    function_types=['linear', 'sin'],\n",
        "    noise_type='homoscedastic',\n",
        "    train_range=train_range,\n",
        "    sampling_regions=sampling_regions,\n",
        "    n_train=n_train,\n",
        "    grid_points=grid_points,\n",
        "    seed=seed,\n",
        "    n_iter=12000,\n",
        "    burnin=2000,\n",
        "    thin=10,\n",
        "    nsamples=1000,\n",
        "    entropy_method='analytical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BAMLSS - Heteroscedastic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_bamlss_undersampling_experiment(\n",
        "    generate_toy_regression_func=generate_toy_regression,\n",
        "    function_types=['linear', 'sin'],\n",
        "    noise_type='heteroscedastic',\n",
        "    train_range=train_range,\n",
        "    sampling_regions=sampling_regions,\n",
        "    n_train=n_train,\n",
        "    grid_points=grid_points,\n",
        "    seed=seed,\n",
        "    n_iter=12000,\n",
        "    burnin=2000,\n",
        "    thin=10,\n",
        "    nsamples=1000,\n",
        "    entropy_method='analytical'\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
