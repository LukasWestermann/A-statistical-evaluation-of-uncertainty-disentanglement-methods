{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "946b4b4e",
      "metadata": {},
      "source": [
        "# Omitted Variable Bias (OVB) - Regression Demo\n",
        "\n",
        "## Data-Generating Process\n",
        "\n",
        "### True Underlying Model\n",
        "\n",
        "$$Y = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\varepsilon$$\n",
        "\n",
        "where:\n",
        "- $Y$: continuous outcome\n",
        "- $X$: observed explanatory variable  \n",
        "- $Z$: omitted (latent) variable that also affects $Y$\n",
        "- $\\varepsilon \\sim N(0, \\sigma^2)$: random noise\n",
        "- $\\beta_0, \\beta_1, \\beta_2$: true coefficients\n",
        "\n",
        "### Constructing Correlation Between X and Z\n",
        "\n",
        "To control the degree of confounding, we define $X$ as a noisy version of $Z$:\n",
        "\n",
        "$$X = \\rho Z + \\sqrt{1 - \\rho^2} \\cdot \\nu$$\n",
        "\n",
        "where:\n",
        "- $\\rho$ is the correlation between $X$ and $Z$ (ranging from 0 to 1)\n",
        "- $\\nu \\sim N(0, 1)$ is independent noise\n",
        "\n",
        "This guarantees that $\\text{Corr}(X, Z) = \\rho$.\n",
        "\n",
        "### Expected Behavior\n",
        "\n",
        "- When $Z$ strongly influences $Y$ and is correlated with $X$, omitting $Z$ biases the estimated effect of $X$\n",
        "- As $\\rho$ increases, bias grows but the model becomes \"confidently wrong\"\n",
        "- Adding more data reduces variance but **not** the bias itself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ed3672",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2e9bf61",
      "metadata": {},
      "source": [
        "## Data Generating Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c063f34",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_toy_regression_ovb(\n",
        "    n_train: int = 1000,\n",
        "    train_range: tuple = (0.0, 10.0),\n",
        "    grid_points: int = 1000,\n",
        "    noise_type: str = 'heteroscedastic',\n",
        "    func_type: str = 'linear',\n",
        "    rho: float = 0.7,\n",
        "    beta2: float = 1.0,\n",
        "    seed: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate toy regression data with OVB potential.\n",
        "    \n",
        "    Extends the standard regression DGP with a correlated latent variable Z.\n",
        "    Model: Y = f(X) + beta2*Z + epsilon\n",
        "    Where X and Z are correlated via rho.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_train : int\n",
        "        Number of training samples\n",
        "    train_range : tuple\n",
        "        (min, max) range for X values (used for grid)\n",
        "    grid_points : int\n",
        "        Number of grid points for evaluation\n",
        "    noise_type : str\n",
        "        'homoscedastic' or 'heteroscedastic'\n",
        "    func_type : str\n",
        "        'linear' (f(x) = 0.7x + 0.5) or 'sin' (f(x) = x*sin(x) + x)\n",
        "    rho : float\n",
        "        Correlation between X and Z (controls confounding strength)\n",
        "    beta2 : float\n",
        "        Effect of omitted variable Z on Y\n",
        "    seed : int\n",
        "        Random seed for reproducibility\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    X, Z, Y, x_grid, y_grid_clean : tuple\n",
        "        Training data (X, Z, Y) and evaluation grid\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    \n",
        "    # Generate latent Z ~ N(0, 1)\n",
        "    Z = rng.standard_normal(n_train)\n",
        "    \n",
        "    # Generate X correlated with Z: X = rho*Z + sqrt(1-rho^2)*nu\n",
        "    # This ensures Corr(X, Z) = rho\n",
        "    nu = rng.standard_normal(n_train)\n",
        "    X = rho * Z + np.sqrt(1 - rho**2) * nu\n",
        "    \n",
        "    # Scale X to desired range (shift and scale from ~N(0,1) to train_range)\n",
        "    X_scaled = X * (train_range[1] - train_range[0]) / 4 + (train_range[0] + train_range[1]) / 2\n",
        "    \n",
        "    # Define the function f(x)\n",
        "    if func_type == 'linear':\n",
        "        f_clean = lambda x: 0.7 * x + 0.5\n",
        "    elif func_type == 'sin':\n",
        "        f_clean = lambda x: x * np.sin(x) + x\n",
        "    else:\n",
        "        raise ValueError(\"func_type must be 'linear' or 'sin'\")\n",
        "    \n",
        "    # Compute clean function output\n",
        "    y_clean = f_clean(X_scaled)\n",
        "    \n",
        "    # Generate noise epsilon\n",
        "    if noise_type == 'homoscedastic':\n",
        "        sigma = 1.0\n",
        "        epsilon = rng.normal(0, sigma, n_train)\n",
        "    elif noise_type == 'heteroscedastic':\n",
        "        sigma = np.abs(2.5 * np.sin(0.5 * X_scaled + 5))\n",
        "        epsilon = rng.normal(0, sigma)\n",
        "    else:\n",
        "        raise ValueError(\"noise_type must be 'homoscedastic' or 'heteroscedastic'\")\n",
        "    \n",
        "    # Generate Y with OVB structure: Y = f(X) + beta2*Z + epsilon\n",
        "    Y = y_clean + beta2 * Z + epsilon\n",
        "    \n",
        "    # Create evaluation grid\n",
        "    x_grid = np.linspace(train_range[0], train_range[1], grid_points)\n",
        "    y_grid_clean = f_clean(x_grid)\n",
        "    \n",
        "    return X_scaled, Z, Y, x_grid, y_grid_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971a7cd6",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79aa189f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment configuration\n",
        "cfg = {\n",
        "    \"n_train\": 1000,\n",
        "    \"train_range\": (0.0, 10.0),\n",
        "    \"grid_points\": 1000,\n",
        "    \"noise_type\": \"heteroscedastic\",  # 'homoscedastic' or 'heteroscedastic'\n",
        "    \"func_type\": \"linear\",            # 'linear' or 'sin'\n",
        "    \"rho\": 0.7,                       # Correlation between X and Z\n",
        "    \"beta2\": 1.0,                     # Effect of omitted Z on Y\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "print(\"Experiment Parameters:\")\n",
        "print(f\"  n_train: {cfg['n_train']}\")\n",
        "print(f\"  train_range: {cfg['train_range']}\")\n",
        "print(f\"  noise_type: {cfg['noise_type']}\")\n",
        "print(f\"  func_type: {cfg['func_type']}\")\n",
        "print(f\"  rho (X-Z correlation): {cfg['rho']}\")\n",
        "print(f\"  beta2 (effect of Z): {cfg['beta2']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9136ad25",
      "metadata": {},
      "source": [
        "## Generate Data and Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7966bec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data using the toy regression OVB function\n",
        "X, Z, Y, x_grid, y_grid_clean = generate_toy_regression_ovb(**cfg)\n",
        "\n",
        "# Verify empirical correlation\n",
        "empirical_corr = np.corrcoef(X, Z)[0, 1]\n",
        "print(f\"Empirical correlation between X and Z: {empirical_corr:.4f} (target: {cfg['rho']})\")\n",
        "print(f\"Function type: {cfg['func_type']}, Noise type: {cfg['noise_type']}\")\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# X vs Y with clean function\n",
        "axes[0, 0].scatter(X, Y, alpha=0.5, s=10, label='Data')\n",
        "axes[0, 0].plot(x_grid, y_grid_clean, 'r-', linewidth=2, label='f(x) clean')\n",
        "axes[0, 0].set_xlabel('X (Observed)')\n",
        "axes[0, 0].set_ylabel('Y (Outcome)')\n",
        "axes[0, 0].set_title('X vs Y')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Z vs Y\n",
        "axes[0, 1].scatter(Z, Y, alpha=0.5, s=10, color='orange')\n",
        "axes[0, 1].set_xlabel('Z (Omitted/Latent)')\n",
        "axes[0, 1].set_ylabel('Y (Outcome)')\n",
        "axes[0, 1].set_title('Z vs Y')\n",
        "\n",
        "# X vs Z\n",
        "axes[1, 0].scatter(X, Z, alpha=0.5, s=10, color='green')\n",
        "axes[1, 0].set_xlabel('X (Observed)')\n",
        "axes[1, 0].set_ylabel('Z (Omitted/Latent)')\n",
        "axes[1, 0].set_title(f'X vs Z (Corr = {empirical_corr:.3f})')\n",
        "\n",
        "# Distribution of Z\n",
        "axes[1, 1].hist(Z, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[1, 1].set_xlabel('Z (Omitted/Latent)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Distribution of Z ~ N(0, 1)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d88183",
      "metadata": {},
      "source": [
        "## Biased Regression (Omitting Z)\n",
        "\n",
        "When we regress Y on X only (omitting Z), we get a biased estimate of $\\beta_1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbddd787",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define true beta1 based on function type\n",
        "# For linear: f(x) = 0.7*x + 0.5, so true coefficient on X is 0.7\n",
        "true_beta1 = 0.7 if cfg['func_type'] == 'linear' else None\n",
        "\n",
        "# Fit biased model: Y ~ X (omitting Z)\n",
        "X_with_const = sm.add_constant(X)\n",
        "model_biased = sm.OLS(Y, X_with_const).fit()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BIASED MODEL: Y ~ X (Z omitted)\")\n",
        "print(\"=\" * 60)\n",
        "print(model_biased.summary())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COEFFICIENT COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Estimated beta1: {model_biased.params[1]:.4f}\")\n",
        "print(f\"True beta1:      {true_beta1:.4f}\" if true_beta1 else \"True beta1: N/A (nonlinear)\")\n",
        "print(f\"Bias:            {model_biased.params[1] - true_beta1:.4f}\" if true_beta1 else \"Bias: N/A\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eca23af4",
      "metadata": {},
      "source": [
        "## Unbiased Regression (Including Z)\n",
        "\n",
        "When we include Z in the regression, we recover the true coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb525bed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit unbiased model: Y ~ X + Z (full model)\n",
        "XZ = np.column_stack([X, Z])\n",
        "XZ_with_const = sm.add_constant(XZ)\n",
        "model_unbiased = sm.OLS(Y, XZ_with_const).fit()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"UNBIASED MODEL: Y ~ X + Z\")\n",
        "print(\"=\" * 60)\n",
        "print(model_unbiased.summary())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COEFFICIENT COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "if true_beta1:\n",
        "    print(f\"Estimated beta1 (X): {model_unbiased.params[1]:.4f} (True: {true_beta1})\")\n",
        "    print(f\"Estimated beta2 (Z): {model_unbiased.params[2]:.4f} (True: {cfg['beta2']})\")\n",
        "    print(f\"Beta1 bias: {model_unbiased.params[1] - true_beta1:.4f}\")\n",
        "    print(f\"Beta2 bias: {model_unbiased.params[2] - cfg['beta2']:.4f}\")\n",
        "else:\n",
        "    print(f\"Estimated beta1 (X): {model_unbiased.params[1]:.4f} (True: N/A - nonlinear)\")\n",
        "    print(f\"Estimated beta2 (Z): {model_unbiased.params[2]:.4f} (True: {cfg['beta2']})\")\n",
        "    print(f\"Beta2 bias: {model_unbiased.params[2] - cfg['beta2']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ba77bb",
      "metadata": {},
      "source": [
        "## Theoretical Bias Analysis\n",
        "\n",
        "The OVB formula shows that the bias in the estimated $\\beta_1$ when omitting $Z$ is:\n",
        "\n",
        "$$\\text{Bias}(\\hat{\\beta}_1) = \\beta_2 \\cdot \\frac{\\text{Cov}(X, Z)}{\\text{Var}(X)} = \\beta_2 \\cdot \\rho \\cdot \\frac{\\sigma_Z}{\\sigma_X}$$\n",
        "\n",
        "Since we constructed $X$ and $Z$ to have unit variance, this simplifies to:\n",
        "$$\\text{Bias}(\\hat{\\beta}_1) \\approx \\beta_2 \\cdot \\rho$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e075e2e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute theoretical vs observed bias\n",
        "theoretical_bias = cfg['beta2'] * cfg['rho']  # Approximate formula\n",
        "observed_bias = model_biased.params[1] - true_beta1 if true_beta1 else np.nan\n",
        "\n",
        "# More precise formula using actual variances\n",
        "cov_xz = np.cov(X, Z)[0, 1]\n",
        "var_x = np.var(X)\n",
        "precise_theoretical_bias = cfg['beta2'] * cov_xz / var_x\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BIAS ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Theoretical bias (approx): beta2 * rho = {cfg['beta2']} * {cfg['rho']} = {theoretical_bias:.4f}\")\n",
        "print(f\"Theoretical bias (precise): beta2 * Cov(X,Z)/Var(X) = {precise_theoretical_bias:.4f}\")\n",
        "if true_beta1:\n",
        "    print(f\"Observed bias: {observed_bias:.4f}\")\n",
        "    print(f\"Difference: {abs(observed_bias - precise_theoretical_bias):.4f}\")\n",
        "else:\n",
        "    print(\"Observed bias: N/A (nonlinear function)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2e4964",
      "metadata": {},
      "source": [
        "## Experiment: Varying Correlation (rho)\n",
        "\n",
        "Let's see how the bias changes as we vary the correlation between X and Z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e0a02d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment: vary rho and observe bias\n",
        "rho_values = [0.0, 0.2, 0.4, 0.6, 0.8, 0.95]\n",
        "results = []\n",
        "\n",
        "# True beta1 depends on func_type: linear = 0.7\n",
        "true_beta1 = 0.7 if cfg['func_type'] == 'linear' else None  # sin is nonlinear\n",
        "\n",
        "for rho in rho_values:\n",
        "    # Generate data with this rho\n",
        "    X_exp, Z_exp, Y_exp, _, _ = generate_toy_regression_ovb(\n",
        "        n_train=cfg['n_train'],\n",
        "        train_range=cfg['train_range'],\n",
        "        grid_points=cfg['grid_points'],\n",
        "        noise_type=cfg['noise_type'],\n",
        "        func_type=cfg['func_type'],\n",
        "        rho=rho,\n",
        "        beta2=cfg['beta2'],\n",
        "        seed=cfg['seed']\n",
        "    )\n",
        "    \n",
        "    # Fit biased model\n",
        "    X_exp_const = sm.add_constant(X_exp)\n",
        "    model_exp = sm.OLS(Y_exp, X_exp_const).fit()\n",
        "    \n",
        "    # Store results (for linear func_type, compare to 0.7)\n",
        "    results.append({\n",
        "        'rho': rho,\n",
        "        'estimated_beta1': model_exp.params[1],\n",
        "        'true_beta1': true_beta1,\n",
        "        'bias': model_exp.params[1] - true_beta1 if true_beta1 else np.nan,\n",
        "        'theoretical_bias': cfg['beta2'] * rho,\n",
        "        'std_err': model_exp.bse[1],\n",
        "        'ci_lower': model_exp.conf_int()[1, 0],\n",
        "        'ci_upper': model_exp.conf_int()[1, 1],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2067e25c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize how bias grows with rho\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Estimated beta1 vs rho\n",
        "ax1 = axes[0]\n",
        "ax1.errorbar(results_df['rho'], results_df['estimated_beta1'], \n",
        "             yerr=1.96 * results_df['std_err'], \n",
        "             fmt='o-', capsize=5, label='Estimated beta1 (95% CI)', color='blue')\n",
        "if true_beta1:\n",
        "    ax1.axhline(y=true_beta1, color='green', linestyle='--', linewidth=2, label=f\"True beta1 = {true_beta1}\")\n",
        "ax1.set_xlabel('rho (X-Z Correlation)')\n",
        "ax1.set_ylabel('Estimated beta1')\n",
        "ax1.set_title('Estimated Coefficient vs. Confounding Strength')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Observed vs Theoretical bias\n",
        "ax2 = axes[1]\n",
        "if true_beta1:\n",
        "    ax2.plot(results_df['rho'], results_df['bias'], 'o-', label='Observed Bias', color='red', linewidth=2)\n",
        "ax2.plot(results_df['rho'], results_df['theoretical_bias'], 's--', label='Theoretical Bias (beta2 * rho)', color='orange', linewidth=2)\n",
        "ax2.axhline(y=0, color='gray', linestyle='-', linewidth=1)\n",
        "ax2.set_xlabel('rho (X-Z Correlation)')\n",
        "ax2.set_ylabel('Bias in beta1 Estimate')\n",
        "ax2.set_title('OVB: Bias Grows with Correlation')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783a24d5",
      "metadata": {},
      "source": [
        "## Summary: Biased vs Unbiased Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb8fef4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary comparison table\n",
        "if true_beta1:\n",
        "    summary_data = {\n",
        "        'Model': ['Biased (Y ~ X)', 'Unbiased (Y ~ X + Z)'],\n",
        "        'beta1 Estimate': [model_biased.params[1], model_unbiased.params[1]],\n",
        "        'beta1 True': [true_beta1, true_beta1],\n",
        "        'beta1 Bias': [model_biased.params[1] - true_beta1, model_unbiased.params[1] - true_beta1],\n",
        "        'beta1 Std Err': [model_biased.bse[1], model_unbiased.bse[1]],\n",
        "        'beta1 95% CI': [\n",
        "            f\"[{model_biased.conf_int()[1, 0]:.3f}, {model_biased.conf_int()[1, 1]:.3f}]\",\n",
        "            f\"[{model_unbiased.conf_int()[1, 0]:.3f}, {model_unbiased.conf_int()[1, 1]:.3f}]\"\n",
        "        ],\n",
        "        'R-squared': [model_biased.rsquared, model_unbiased.rsquared],\n",
        "    }\n",
        "    \n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SUMMARY: BIASED vs UNBIASED REGRESSION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Configuration: n={cfg['n_train']}, func_type={cfg['func_type']}, beta2={cfg['beta2']}, rho={cfg['rho']}\")\n",
        "    print()\n",
        "    print(summary_df.to_string(index=False))\n",
        "    print()\n",
        "    print(\"KEY INSIGHT:\")\n",
        "    print(f\"  - The biased model overestimates beta1 by {model_biased.params[1] - true_beta1:.3f}\")\n",
        "    print(f\"  - This is because it attributes Z's effect to X (since they're correlated)\")\n",
        "    print(f\"  - The confidence interval for the biased estimate does NOT contain the true value!\")\n",
        "else:\n",
        "    print(\"Summary not available for nonlinear function types.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d29b8f4",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Uncertainty Estimation with MC Dropout\n",
        "\n",
        "Now we use MC Dropout to estimate aleatoric and epistemic uncertainty when training on X only (omitting Z). We analyze how these uncertainties change as we vary:\n",
        "1. **rho** (correlation between X and Z) - with fixed beta2\n",
        "2. **beta2** (effect of omitted Z) - with fixed rho\n",
        "\n",
        "Both variance-based and entropy-based (IT) decompositions are computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89264673",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional imports for uncertainty experiments\n",
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'Experiments' else Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Setup results directory\n",
        "results_dir = project_root / \"results\" / \"ovb\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Results will be saved to: {results_dir}\")\n",
        "\n",
        "# Import OVB experiment functions\n",
        "from utils.ovb_experiments import (\n",
        "    # MC Dropout\n",
        "    run_mc_dropout_ovb_rho_experiment,\n",
        "    run_mc_dropout_ovb_beta2_experiment,\n",
        "    # Deep Ensemble\n",
        "    run_deep_ensemble_ovb_rho_experiment,\n",
        "    run_deep_ensemble_ovb_beta2_experiment,\n",
        "    # BNN\n",
        "    run_bnn_ovb_rho_experiment,\n",
        "    run_bnn_ovb_beta2_experiment,\n",
        "    # BAMLSS\n",
        "    run_bamlss_ovb_rho_experiment,\n",
        "    run_bamlss_ovb_beta2_experiment,\n",
        "    # Plotting\n",
        "    plot_ovb_uncertainty_comparison,\n",
        "    plot_ovb_heatmap_comparison,\n",
        "    plot_ovb_marginal_comparison,\n",
        "    plot_ovb_z_slices\n",
        ")\n",
        "\n",
        "from utils.device import get_device\n",
        "device = get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d00ed3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# MC Dropout hyperparameters\n",
        "mc_cfg = {\n",
        "    \"n_train\": 1000,\n",
        "    \"train_range\": (-5, 15),\n",
        "    \"grid_points\": 600,\n",
        "    \"func_type\": [\"linear\", \"sin\"],\n",
        "    \"noise_type\": [\"homoscedastic\", \"heteroscedastic\"],\n",
        "    \"seed\": 42,\n",
        "    # MC Dropout specific\n",
        "    \"p\": 0.25,           # Dropout probability\n",
        "    \"beta\": 0.5,         # Beta-NLL beta parameter\n",
        "    \"epochs\": 500,\n",
        "    \"lr\": 1e-3,\n",
        "    \"batch_size\": 32,\n",
        "    \"mc_samples\": 100,\n",
        "    \"entropy_method\": \"analytical\",\n",
        "}\n",
        "\n",
        "print(\"MC Dropout Configuration:\")\n",
        "for k, v in mc_cfg.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21dc9ed3",
      "metadata": {},
      "source": [
        "## Experiment 1: Varying rho (X-Z Correlation)\n",
        "\n",
        "Fix beta2 and vary rho to see how increasing correlation between X and Z affects uncertainty estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9e754e19",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m rho_values = [\u001b[32m0.0\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m1\u001b[39m]\n\u001b[32m      3\u001b[39m fixed_beta2 = \u001b[32m1.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m rho_results_df, rho_all_results = \u001b[43mrun_mc_dropout_ovb_rho_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrho_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrho_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_beta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_range\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrid_points\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunc_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnoise_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmc_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmc_samples\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentropy_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mentropy_method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_plots\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_dir\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\OneDrive\\Desktop\\Code-Masterarbeit\\A-statistical-evaluation-of-uncertainty-disentanglement-methods-1\\utils\\ovb_experiments.py:341\u001b[39m, in \u001b[36mrun_mc_dropout_ovb_rho_experiment\u001b[39m\u001b[34m(rho_values, beta2, n_train, train_range, grid_points, func_type, noise_type, seed, p, beta, epochs, lr, batch_size, mc_samples, entropy_method, parallel, save_plots, results_dir)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning config: func_type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mft\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, noise_type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    339\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m df, results = \u001b[43mrun_mc_dropout_ovb_rho_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrho_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrho_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmc_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmc_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentropy_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentropy_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_plots\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_plots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_dir\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# Add config columns to dataframe\u001b[39;00m\n\u001b[32m    363\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mfunc_type\u001b[39m\u001b[33m'\u001b[39m] = ft\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\OneDrive\\Desktop\\Code-Masterarbeit\\A-statistical-evaluation-of-uncertainty-disentanglement-methods-1\\utils\\ovb_experiments.py:455\u001b[39m, in \u001b[36mrun_mc_dropout_ovb_rho_experiment\u001b[39m\u001b[34m(rho_values, beta2, n_train, train_range, grid_points, func_type, noise_type, seed, p, beta, epochs, lr, batch_size, mc_samples, entropy_method, parallel, save_plots, results_dir)\u001b[39m\n\u001b[32m    453\u001b[39m ds_full = TensorDataset(torch.from_numpy(X_full), torch.from_numpy(Y))\n\u001b[32m    454\u001b[39m loader_full = DataLoader(ds_full, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbeta_nll\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Make predictions on training points (X, Z)\u001b[39;00m\n\u001b[32m    458\u001b[39m result_full = mc_dropout_predict(model_full, X_full, M=mc_samples, return_raw_arrays=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\OneDrive\\Desktop\\Code-Masterarbeit\\A-statistical-evaluation-of-uncertainty-disentanglement-methods-1\\Models\\MC_Dropout.py:77\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, loader, epochs, lr, loss_type, beta, device)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mloss_type must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnll\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbeta_nll\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m loss.backward()\n\u001b[32m     79\u001b[39m opt.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\OneDrive\\Desktop\\Code-Masterarbeit\\A-statistical-evaluation-of-uncertainty-disentanglement-methods-1\\venv\\Lib\\site-packages\\torch\\_compile.py:53\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\OneDrive\\Desktop\\Code-Masterarbeit\\A-statistical-evaluation-of-uncertainty-disentanglement-methods-1\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1044\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1046\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lukas\\OneDrive\\Desktop\\Code-Masterarbeit\\A-statistical-evaluation-of-uncertainty-disentanglement-methods-1\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:1035\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m         p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Run rho experiment\n",
        "rho_values = [0.0, 0.5, 1]\n",
        "fixed_beta2 = 1.0\n",
        "\n",
        "rho_results_df, rho_all_results = run_mc_dropout_ovb_rho_experiment(\n",
        "    rho_values=rho_values,\n",
        "    beta2=fixed_beta2,\n",
        "    n_train=mc_cfg[\"n_train\"],\n",
        "    train_range=mc_cfg[\"train_range\"],\n",
        "    grid_points=mc_cfg[\"grid_points\"],\n",
        "    func_type=mc_cfg[\"func_type\"],\n",
        "    noise_type=mc_cfg[\"noise_type\"],\n",
        "    seed=mc_cfg[\"seed\"],\n",
        "    p=mc_cfg[\"p\"],\n",
        "    beta=mc_cfg[\"beta\"],\n",
        "    epochs=mc_cfg[\"epochs\"],\n",
        "    lr=mc_cfg[\"lr\"],\n",
        "    batch_size=mc_cfg[\"batch_size\"],\n",
        "    mc_samples=mc_cfg[\"mc_samples\"],\n",
        "    entropy_method=mc_cfg[\"entropy_method\"],\n",
        "    save_plots=True,\n",
        "    results_dir=results_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7302760",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display rho experiment results\n",
        "print(\"=\" * 80)\n",
        "print(\"RHO EXPERIMENT RESULTS (Varying X-Z Correlation)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nFixed beta2 = {fixed_beta2}\")\n",
        "print()\n",
        "print(rho_results_df.to_string(index=False))\n",
        "\n",
        "# Bar plot comparison\n",
        "plot_ovb_uncertainty_comparison(rho_results_df, vary_param='rho')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cabf406",
      "metadata": {},
      "source": [
        "### Detailed Heatmap and Marginal Visualizations (Rho Experiment)\n",
        "\n",
        "Compare full model (with Z) vs omitted model (X only) using 2D heatmaps and marginal distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62961a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap and marginal visualizations for each rho value\n",
        "for rho_val in rho_values:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Visualizations for rho = {rho_val}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # 2D Heatmaps - Variance decomposition\n",
        "    plot_ovb_heatmap_comparison(\n",
        "        rho_all_results, \n",
        "        param_value=rho_val, \n",
        "        decomposition='variance',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # 2D Heatmaps - Entropy decomposition\n",
        "    plot_ovb_heatmap_comparison(\n",
        "        rho_all_results, \n",
        "        param_value=rho_val, \n",
        "        decomposition='entropy',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # Marginal distributions - Variance\n",
        "    plot_ovb_marginal_comparison(\n",
        "        rho_all_results, \n",
        "        param_value=rho_val, \n",
        "        decomposition='variance',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # Marginal distributions - Entropy\n",
        "    plot_ovb_marginal_comparison(\n",
        "        rho_all_results, \n",
        "        param_value=rho_val, \n",
        "        decomposition='entropy',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # Z-slice comparison - Variance\n",
        "    plot_ovb_z_slices(\n",
        "        rho_all_results,\n",
        "        param_value=rho_val,\n",
        "        z_percentiles=[10, 50, 90],\n",
        "        decomposition='variance',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # Z-slice comparison - Entropy\n",
        "    plot_ovb_z_slices(\n",
        "        rho_all_results,\n",
        "        param_value=rho_val,\n",
        "        z_percentiles=[10, 50, 90],\n",
        "        decomposition='entropy',\n",
        "        results_dir=results_dir\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d95677",
      "metadata": {},
      "source": [
        "## Experiment 2: Varying beta2 (Effect of Omitted Z)\n",
        "\n",
        "Fix rho and vary beta2 to see how increasing the effect of the omitted variable Z affects uncertainty estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31b451ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run beta2 experiment\n",
        "beta2_values = [0.0, 0.5, 1.0, 2.0, 3.0]\n",
        "fixed_rho = 0.7\n",
        "\n",
        "beta2_results_df, beta2_all_results = run_mc_dropout_ovb_beta2_experiment(\n",
        "    beta2_values=beta2_values,\n",
        "    rho=fixed_rho,\n",
        "    n_train=mc_cfg[\"n_train\"],\n",
        "    train_range=mc_cfg[\"train_range\"],\n",
        "    grid_points=mc_cfg[\"grid_points\"],\n",
        "    func_type=mc_cfg[\"func_type\"],\n",
        "    noise_type=mc_cfg[\"noise_type\"],\n",
        "    seed=mc_cfg[\"seed\"],\n",
        "    p=mc_cfg[\"p\"],\n",
        "    beta=mc_cfg[\"beta\"],\n",
        "    epochs=mc_cfg[\"epochs\"],\n",
        "    lr=mc_cfg[\"lr\"],\n",
        "    batch_size=mc_cfg[\"batch_size\"],\n",
        "    mc_samples=mc_cfg[\"mc_samples\"],\n",
        "    entropy_method=mc_cfg[\"entropy_method\"],\n",
        "    save_plots=True,\n",
        "    results_dir=results_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a3ad27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display beta2 experiment results\n",
        "print(\"=\" * 80)\n",
        "print(\"BETA2 EXPERIMENT RESULTS (Varying Effect of Omitted Z)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nFixed rho = {fixed_rho}\")\n",
        "print()\n",
        "print(beta2_results_df.to_string(index=False))\n",
        "\n",
        "# Bar plot comparison\n",
        "plot_ovb_uncertainty_comparison(beta2_results_df, vary_param='beta2')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ae44fe",
      "metadata": {},
      "source": [
        "### Detailed Heatmap and Marginal Visualizations (Beta2 Experiment)\n",
        "\n",
        "Compare full model (with Z) vs omitted model (X only) using 2D heatmaps and marginal distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f3c42b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap and marginal visualizations for each beta2 value\n",
        "for beta2_val in beta2_values:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Visualizations for beta2 = {beta2_val}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # 2D Heatmaps - Variance decomposition\n",
        "    plot_ovb_heatmap_comparison(\n",
        "        beta2_all_results, \n",
        "        param_value=beta2_val, \n",
        "        decomposition='variance',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # 2D Heatmaps - Entropy decomposition\n",
        "    plot_ovb_heatmap_comparison(\n",
        "        beta2_all_results, \n",
        "        param_value=beta2_val, \n",
        "        decomposition='entropy',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # Marginal distributions - Variance\n",
        "    plot_ovb_marginal_comparison(\n",
        "        beta2_all_results, \n",
        "        param_value=beta2_val, \n",
        "        decomposition='variance',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # Marginal distributions - Entropy\n",
        "    plot_ovb_marginal_comparison(\n",
        "        beta2_all_results, \n",
        "        param_value=beta2_val, \n",
        "        decomposition='entropy',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # Z-slice comparison - Variance\n",
        "    plot_ovb_z_slices(\n",
        "        beta2_all_results,\n",
        "        param_value=beta2_val,\n",
        "        z_percentiles=[10, 50, 90],\n",
        "        decomposition='variance',\n",
        "        results_dir=results_dir\n",
        "    )\n",
        "    \n",
        "    # Z-slice comparison - Entropy\n",
        "    plot_ovb_z_slices(\n",
        "        beta2_all_results,\n",
        "        param_value=beta2_val,\n",
        "        z_percentiles=[10, 50, 90],\n",
        "        decomposition='entropy',\n",
        "        results_dir=results_dir\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1320dd",
      "metadata": {},
      "source": [
        "## Summary: OVB Uncertainty Experiments\n",
        "\n",
        "### Key Observations\n",
        "\n",
        "**Varying rho (X-Z Correlation):**\n",
        "- As rho increases, X and Z become more correlated\n",
        "- The model trained on X only cannot distinguish X's effect from Z's confounded effect\n",
        "- Epistemic uncertainty may change as the model becomes more/less confident\n",
        "\n",
        "**Varying beta2 (Effect of Omitted Z):**\n",
        "- As beta2 increases, the omitted variable Z has a larger effect on Y\n",
        "- This introduces more unexplained variance from the model's perspective\n",
        "- Both aleatoric and epistemic uncertainty estimates may be affected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65546a11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combined summary statistics\n",
        "print(\"=\" * 80)\n",
        "print(\"COMBINED SUMMARY: OVB UNCERTAINTY EXPERIMENTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n--- Rho Experiment (fixed beta2={}) ---\".format(fixed_beta2))\n",
        "print(\"Variance Decomposition:\")\n",
        "print(f\"  AU range: {rho_results_df['mean_ale_var'].min():.4f} - {rho_results_df['mean_ale_var'].max():.4f}\")\n",
        "print(f\"  EU range: {rho_results_df['mean_epi_var'].min():.4f} - {rho_results_df['mean_epi_var'].max():.4f}\")\n",
        "print(\"Entropy Decomposition:\")\n",
        "print(f\"  AU range: {rho_results_df['mean_ale_entropy'].min():.4f} - {rho_results_df['mean_ale_entropy'].max():.4f}\")\n",
        "print(f\"  EU range: {rho_results_df['mean_epi_entropy'].min():.4f} - {rho_results_df['mean_epi_entropy'].max():.4f}\")\n",
        "\n",
        "print(\"\\n--- Beta2 Experiment (fixed rho={}) ---\".format(fixed_rho))\n",
        "print(\"Variance Decomposition:\")\n",
        "print(f\"  AU range: {beta2_results_df['mean_ale_var'].min():.4f} - {beta2_results_df['mean_ale_var'].max():.4f}\")\n",
        "print(f\"  EU range: {beta2_results_df['mean_epi_var'].min():.4f} - {beta2_results_df['mean_epi_var'].max():.4f}\")\n",
        "print(\"Entropy Decomposition:\")\n",
        "print(f\"  AU range: {beta2_results_df['mean_ale_entropy'].min():.4f} - {beta2_results_df['mean_ale_entropy'].max():.4f}\")\n",
        "print(f\"  EU range: {beta2_results_df['mean_epi_entropy'].min():.4f} - {beta2_results_df['mean_epi_entropy'].max():.4f}\")\n",
        "\n",
        "# Save combined results to CSV\n",
        "rho_results_df.to_csv(results_dir / \"ovb_rho_experiment_results.csv\", index=False)\n",
        "beta2_results_df.to_csv(results_dir / \"ovb_beta2_experiment_results.csv\", index=False)\n",
        "print(f\"\\nResults saved to {results_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a74a9fef",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Deep Ensemble OVB Experiments\n",
        "\n",
        "Deep Ensembles provide uncertainty estimates through the disagreement between ensemble members.\n",
        "Training K independent networks and combining their predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9e0377",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deep Ensemble hyperparameters\n",
        "de_cfg = {\n",
        "    \"n_train\": 1000,\n",
        "    \"train_range\": (-5, 15),\n",
        "    \"grid_points\": 600,\n",
        "    \"func_type\": [\"linear\", \"sin\"],\n",
        "    \"noise_type\": [\"homoscedastic\", \"heteroscedastic\"],\n",
        "    \"seed\": 42,\n",
        "    # Deep Ensemble specific\n",
        "    \"K\": 10,               # Number of ensemble members\n",
        "    \"epochs\": 500,\n",
        "    \"batch_size\": 32,\n",
        "    \"entropy_method\": \"analytical\",\n",
        "}\n",
        "\n",
        "print(\"Deep Ensemble Configuration:\")\n",
        "for k, v in de_cfg.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553b0c21",
      "metadata": {},
      "source": [
        "## Deep Ensemble: Varying rho (X-Z Correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40db287c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Deep Ensemble rho experiment\n",
        "de_rho_values = [0.0, 0.5, 1.0]\n",
        "de_fixed_beta2 = 1.0\n",
        "\n",
        "de_rho_results_df, de_rho_all_results = run_deep_ensemble_ovb_rho_experiment(\n",
        "    rho_values=de_rho_values,\n",
        "    beta2=de_fixed_beta2,\n",
        "    n_train=de_cfg[\"n_train\"],\n",
        "    train_range=de_cfg[\"train_range\"],\n",
        "    grid_points=de_cfg[\"grid_points\"],\n",
        "    func_type=de_cfg[\"func_type\"],\n",
        "    noise_type=de_cfg[\"noise_type\"],\n",
        "    seed=de_cfg[\"seed\"],\n",
        "    K=de_cfg[\"K\"],\n",
        "    epochs=de_cfg[\"epochs\"],\n",
        "    batch_size=de_cfg[\"batch_size\"],\n",
        "    entropy_method=de_cfg[\"entropy_method\"],\n",
        "    save_plots=True,\n",
        "    results_dir=results_dir / \"deep_ensemble\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea6e423",
      "metadata": {},
      "source": [
        "## Deep Ensemble: Varying beta2 (Effect of Omitted Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ea541f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Deep Ensemble beta2 experiment\n",
        "de_beta2_values = [0.0, 0.5, 1.0, 2.0, 3.0]\n",
        "de_fixed_rho = 0.7\n",
        "\n",
        "de_beta2_results_df, de_beta2_all_results = run_deep_ensemble_ovb_beta2_experiment(\n",
        "    beta2_values=de_beta2_values,\n",
        "    rho=de_fixed_rho,\n",
        "    n_train=de_cfg[\"n_train\"],\n",
        "    train_range=de_cfg[\"train_range\"],\n",
        "    grid_points=de_cfg[\"grid_points\"],\n",
        "    func_type=de_cfg[\"func_type\"],\n",
        "    noise_type=de_cfg[\"noise_type\"],\n",
        "    seed=de_cfg[\"seed\"],\n",
        "    K=de_cfg[\"K\"],\n",
        "    epochs=de_cfg[\"epochs\"],\n",
        "    batch_size=de_cfg[\"batch_size\"],\n",
        "    entropy_method=de_cfg[\"entropy_method\"],\n",
        "    save_plots=True,\n",
        "    results_dir=results_dir / \"deep_ensemble\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7570aaa1",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: BNN (Bayesian Neural Network) OVB Experiments\n",
        "\n",
        "BNNs use MCMC sampling (NUTS) to approximate the posterior distribution over weights.\n",
        "This provides principled uncertainty estimates from the weight posterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe97396",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BNN hyperparameters\n",
        "bnn_cfg = {\n",
        "    \"n_train\": 500,        # Smaller for BNN (slower)\n",
        "    \"train_range\": (-5, 15),\n",
        "    \"grid_points\": 300,    # Smaller grid for speed\n",
        "    \"func_type\": [\"linear\", \"sin\"],\n",
        "    \"noise_type\": [\"homoscedastic\", \"heteroscedastic\"],\n",
        "    \"seed\": 42,\n",
        "    # BNN specific\n",
        "    \"hidden_width\": 16,\n",
        "    \"weight_scale\": 1.0,\n",
        "    \"warmup\": 100,         # Reduced for speed\n",
        "    \"samples\": 100,        # Reduced for speed\n",
        "    \"entropy_method\": \"analytical\",\n",
        "}\n",
        "\n",
        "print(\"BNN Configuration:\")\n",
        "for k, v in bnn_cfg.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1f878a",
      "metadata": {},
      "source": [
        "## BNN: Varying rho (X-Z Correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5bd2c2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run BNN rho experiment\n",
        "bnn_rho_values = [0.0, 0.5, 1.0]\n",
        "bnn_fixed_beta2 = 1.0\n",
        "\n",
        "bnn_rho_results_df, bnn_rho_all_results = run_bnn_ovb_rho_experiment(\n",
        "    rho_values=bnn_rho_values,\n",
        "    beta2=bnn_fixed_beta2,\n",
        "    n_train=bnn_cfg[\"n_train\"],\n",
        "    train_range=bnn_cfg[\"train_range\"],\n",
        "    grid_points=bnn_cfg[\"grid_points\"],\n",
        "    func_type=bnn_cfg[\"func_type\"],\n",
        "    noise_type=bnn_cfg[\"noise_type\"],\n",
        "    seed=bnn_cfg[\"seed\"],\n",
        "    hidden_width=bnn_cfg[\"hidden_width\"],\n",
        "    weight_scale=bnn_cfg[\"weight_scale\"],\n",
        "    warmup=bnn_cfg[\"warmup\"],\n",
        "    samples=bnn_cfg[\"samples\"],\n",
        "    entropy_method=bnn_cfg[\"entropy_method\"],\n",
        "    save_plots=True,\n",
        "    results_dir=results_dir / \"bnn\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7617021",
      "metadata": {},
      "source": [
        "## BNN: Varying beta2 (Effect of Omitted Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d61626",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run BNN beta2 experiment\n",
        "bnn_beta2_values = [0.0, 0.5, 1.0, 2.0, 3.0]\n",
        "bnn_fixed_rho = 0.7\n",
        "\n",
        "bnn_beta2_results_df, bnn_beta2_all_results = run_bnn_ovb_beta2_experiment(\n",
        "    beta2_values=bnn_beta2_values,\n",
        "    rho=bnn_fixed_rho,\n",
        "    n_train=bnn_cfg[\"n_train\"],\n",
        "    train_range=bnn_cfg[\"train_range\"],\n",
        "    grid_points=bnn_cfg[\"grid_points\"],\n",
        "    func_type=bnn_cfg[\"func_type\"],\n",
        "    noise_type=bnn_cfg[\"noise_type\"],\n",
        "    seed=bnn_cfg[\"seed\"],\n",
        "    hidden_width=bnn_cfg[\"hidden_width\"],\n",
        "    weight_scale=bnn_cfg[\"weight_scale\"],\n",
        "    warmup=bnn_cfg[\"warmup\"],\n",
        "    samples=bnn_cfg[\"samples\"],\n",
        "    entropy_method=bnn_cfg[\"entropy_method\"],\n",
        "    save_plots=True,\n",
        "    results_dir=results_dir / \"bnn\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91cfacb3",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: BAMLSS (Bayesian GAMLSS) OVB Experiments\n",
        "\n",
        "BAMLSS uses R's bamlss package via rpy2 for Bayesian distributional regression.\n",
        "Requires R and the bamlss package to be installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d101711",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BAMLSS hyperparameters\n",
        "bamlss_cfg = {\n",
        "    \"n_train\": 500,        # Smaller for BAMLSS (slowest)\n",
        "    \"train_range\": (-5, 15),\n",
        "    \"grid_points\": 200,    # Smaller grid for speed\n",
        "    \"func_type\": [\"linear\", \"sin\"],\n",
        "    \"noise_type\": [\"homoscedastic\", \"heteroscedastic\"],\n",
        "    \"seed\": 42,\n",
        "    # BAMLSS specific\n",
        "    \"n_iter\": 6000,        # Reduced for speed\n",
        "    \"burnin\": 1000,\n",
        "    \"thin\": 5,\n",
        "    \"nsamples\": 500,\n",
        "    \"entropy_method\": \"analytical\",\n",
        "}\n",
        "\n",
        "print(\"BAMLSS Configuration:\")\n",
        "for k, v in bamlss_cfg.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c409906",
      "metadata": {},
      "source": [
        "## BAMLSS: Varying rho (X-Z Correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588334a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run BAMLSS rho experiment\n",
        "bamlss_rho_values = [0.0, 0.5, 1.0]\n",
        "bamlss_fixed_beta2 = 1.0\n",
        "\n",
        "bamlss_rho_results_df, bamlss_rho_all_results = run_bamlss_ovb_rho_experiment(\n",
        "    rho_values=bamlss_rho_values,\n",
        "    beta2=bamlss_fixed_beta2,\n",
        "    n_train=bamlss_cfg[\"n_train\"],\n",
        "    train_range=bamlss_cfg[\"train_range\"],\n",
        "    grid_points=bamlss_cfg[\"grid_points\"],\n",
        "    func_type=bamlss_cfg[\"func_type\"],\n",
        "    noise_type=bamlss_cfg[\"noise_type\"],\n",
        "    seed=bamlss_cfg[\"seed\"],\n",
        "    n_iter=bamlss_cfg[\"n_iter\"],\n",
        "    burnin=bamlss_cfg[\"burnin\"],\n",
        "    thin=bamlss_cfg[\"thin\"],\n",
        "    nsamples=bamlss_cfg[\"nsamples\"],\n",
        "    entropy_method=bamlss_cfg[\"entropy_method\"],\n",
        "    save_plots=True,\n",
        "    results_dir=results_dir / \"bamlss\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ac3eb9",
      "metadata": {},
      "source": [
        "## BAMLSS: Varying beta2 (Effect of Omitted Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a19f12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run BAMLSS beta2 experiment\n",
        "bamlss_beta2_values = [0.0, 0.5, 1.0, 2.0, 3.0]\n",
        "bamlss_fixed_rho = 0.7\n",
        "\n",
        "bamlss_beta2_results_df, bamlss_beta2_all_results = run_bamlss_ovb_beta2_experiment(\n",
        "    beta2_values=bamlss_beta2_values,\n",
        "    rho=bamlss_fixed_rho,\n",
        "    n_train=bamlss_cfg[\"n_train\"],\n",
        "    train_range=bamlss_cfg[\"train_range\"],\n",
        "    grid_points=bamlss_cfg[\"grid_points\"],\n",
        "    func_type=bamlss_cfg[\"func_type\"],\n",
        "    noise_type=bamlss_cfg[\"noise_type\"],\n",
        "    seed=bamlss_cfg[\"seed\"],\n",
        "    n_iter=bamlss_cfg[\"n_iter\"],\n",
        "    burnin=bamlss_cfg[\"burnin\"],\n",
        "    thin=bamlss_cfg[\"thin\"],\n",
        "    nsamples=bamlss_cfg[\"nsamples\"],\n",
        "    entropy_method=bamlss_cfg[\"entropy_method\"],\n",
        "    save_plots=True,\n",
        "    results_dir=results_dir / \"bamlss\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
