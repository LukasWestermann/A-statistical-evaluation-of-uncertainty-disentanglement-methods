{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027a449e",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aabd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import Tuple\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import Models\n",
    "# This works for notebooks in the Experiments folder\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'Experiments' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import from Models folder (note: no .py extension needed)\n",
    "from Models.MC_Dropout import (\n",
    "    MCDropoutRegressor,\n",
    "    train_model,\n",
    "    mc_dropout_predict,\n",
    "    gaussian_nll,\n",
    "    beta_nll,\n",
    "    plot_toy_data,\n",
    "    plot_uncertainties,\n",
    "    normalize_x,\n",
    "    normalize_x_data\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cef5df",
   "metadata": {},
   "source": [
    "## Genarte toy data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996b3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ----- Data generation for linear function with homo/heteroscedastic noise -----\n",
    "# f(x) = 0.7x + 0.5\n",
    "# noise_type: 'homoscedastic' (σ(x) = 0.20) or 'heteroscedastic' (σ(x) = 0.10 + 0.2(0.5 + 0.5sin(x)))\n",
    "def generate_toy_regression(n_train=1000, train_range=(0.0, 10.0), \n",
    "                           grid_points=600, noise_type='heteroscedastic', type = \"linear\"):\n",
    "    low, high = train_range\n",
    "    x_train = np.random.uniform(low, high, size=(n_train, 1))\n",
    "    \n",
    "    if type == \"linear\":\n",
    "        # Linear function: f(x) = 0.7x + 0.5\n",
    "        f_clean = lambda x: 0.7 * x + 0.5\n",
    "        y_clean_train = f_clean(x_train)\n",
    "    elif type == \"poly\":\n",
    "        # Polynomial function: f(x) = 0.5 + 0.5x + 0.05x^3\n",
    "        #7 sin(x) + 3| cos(x/2)| with ∼ N (0, 1)\n",
    "        f_clean = lambda x: (x**3) / (x**(2.5))\n",
    "        y_clean_train = f_clean(x_train)\n",
    "    elif type == \"trigonometric\":\n",
    "        #7 sin(x) + 3| cos(x/2)| with ∼ N (0, 1)\n",
    "        f_clean = lambda x: 4 * np.sin(x) + 3 * np.abs(np.cos(x/2))\n",
    "        y_clean_train = f_clean(x_train)\n",
    "    else:\n",
    "        raise ValueError(\"type must be 'linear', 'non-linear'\")\n",
    "\n",
    "    # Define noise variance σ²(x)\n",
    "    if noise_type == 'homoscedastic':\n",
    "        # Homoscedastic: σ(x) = 0.8\n",
    "        sigma = 0.8\n",
    "        sigma_train = np.full_like(x_train, sigma)\n",
    "    elif noise_type == 'heteroscedastic':\n",
    "        # Heteroscedastic: \n",
    "        sigma_train = np.abs(0.5* x_train)\n",
    "    else:\n",
    "        raise ValueError(\"noise_type must be 'homoscedastic' or 'heteroscedastic'\")\n",
    "    \n",
    "    # Generate noise: ε | x ~ N(0, σ²(x))\n",
    "    epsilon = np.random.normal(0.0, sigma_train, size=(n_train, 1))\n",
    "    y_train = y_clean_train + epsilon\n",
    "\n",
    "    # Dense evaluation grid within training range\n",
    "    x_grid = np.linspace(train_range[0], train_range[1], grid_points).reshape(-1, 1)\n",
    "    y_grid_clean = f_clean(x_grid)\n",
    "\n",
    "    return (x_train.astype(np.float32), y_train.astype(np.float32),\n",
    "            x_grid.astype(np.float32), y_grid_clean.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4f031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_toy_data(x_train, y_train, x_grid, y_clean, title=\"Toy Regression Data\"):\n",
    "    \"\"\"Plot the training data and clean function\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot training data points\n",
    "    plt.scatter(x_train, y_train, alpha=0.6, s=20, label=\"Training data\", color='blue')\n",
    "    \n",
    "    # Plot clean function\n",
    "    plt.plot(x_grid, y_clean, 'r--', linewidth=2, label=\"Clean f(x) = 0.7x + 0.5\")\n",
    "    \n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\AppData\\Local\\Temp\\ipykernel_8408\\2976750895.py:21: RuntimeWarning: invalid value encountered in power\n",
      "  f_clean = lambda x: (x**3) / (x**(2.5))\n"
     ]
    }
   ],
   "source": [
    "# Polynomial function with homoscedastic noise\n",
    "x_train_homo, y_train_homo, x_grid_homo, y_clean_homo = generate_toy_regression(\n",
    "    n_train=1000, \n",
    "    train_range=(-5, 5), \n",
    "    noise_type='homoscedastic',\n",
    "    type = \"poly\"\n",
    ")\n",
    "\n",
    "# Polynomial function with heteroscedastic noise (default - used in most experiments)\n",
    "x_train, y_train, x_grid, y_clean = generate_toy_regression(\n",
    "    n_train=1000, \n",
    "    train_range=(-5, 5), \n",
    "    noise_type='heteroscedastic',\n",
    "    type = \"poly\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db67fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_toy_data(x_train_homo, y_train_homo, x_grid_homo, y_clean_homo, title=\"Toy Regression Data Homescedastic (n=1000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_toy_data(x_train, y_train, x_grid, y_clean, title=\"Toy Regression Data Heteroscedastic (n=1000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521eb14",
   "metadata": {},
   "source": [
    "## Function to plot uncertanties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74954ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified plotting function without OOD\n",
    "def plot_uncertainties_no_ood(x_train_subset, y_train_subset, x_grid, y_clean, mu_pred, ale_var, epi_var, tot_var, title):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "    x = x_grid[:, 0]\n",
    "    \n",
    "    # Plot 1: Predictive mean + Total uncertainty\n",
    "    axes[0].scatter(x_train_subset[:, 0], y_train_subset[:, 0], alpha=0.6, s=20, color='blue', label=\"Training data\", zorder=3)\n",
    "    axes[0].plot(x, mu_pred, 'b-', linewidth=2, label=\"Predictive mean\")\n",
    "    axes[0].fill_between(x, mu_pred - np.sqrt(tot_var), mu_pred + np.sqrt(tot_var), \n",
    "                        alpha=0.3, color='blue', label=\"±σ(total)\")\n",
    "    axes[0].plot(x, y_clean[:, 0], 'r--', linewidth=1.5, alpha=0.8, label=\"Clean f(x) = 0.7x + 0.5\")\n",
    "    axes[0].set_ylabel(\"y\")\n",
    "    axes[0].set_title(f\"{title}: Predictive Mean + Total Uncertainty\")\n",
    "    axes[0].legend(loc=\"upper left\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Predictive mean + Aleatoric uncertainty only\n",
    "    axes[1].scatter(x_train_subset[:, 0], y_train_subset[:, 0], alpha=0.6, s=20, color='blue', label=\"Training data\", zorder=3)\n",
    "    axes[1].plot(x, mu_pred, 'b-', linewidth=2, label=\"Predictive mean\")\n",
    "    axes[1].fill_between(x, mu_pred - np.sqrt(ale_var), mu_pred + np.sqrt(ale_var), \n",
    "                        alpha=0.3, color='green', label=\"±σ(aleatoric)\")\n",
    "    axes[1].plot(x, y_clean[:, 0], 'r--', linewidth=1.5, alpha=0.8, label=\"Clean f(x) = 0.7x + 0.5\")\n",
    "    axes[1].set_ylabel(\"y\")\n",
    "    axes[1].set_title(f\"{title}: Predictive Mean + Aleatoric Uncertainty\")\n",
    "    axes[1].legend(loc=\"upper left\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Predictive mean + Epistemic uncertainty only\n",
    "    axes[2].scatter(x_train_subset[:, 0], y_train_subset[:, 0], alpha=0.6, s=20, color='blue', label=\"Training data\", zorder=3)\n",
    "    axes[2].plot(x, mu_pred, 'b-', linewidth=2, label=\"Predictive mean\")\n",
    "    axes[2].fill_between(x, mu_pred - np.sqrt(epi_var), mu_pred + np.sqrt(epi_var), \n",
    "                        alpha=0.3, color='orange', label=\"±σ(epistemic)\")\n",
    "    axes[2].plot(x, y_clean[:, 0], 'r--', linewidth=1.5, alpha=0.8, label=\"Clean f(x) = 0.7x + 0.5\")\n",
    "    axes[2].set_ylabel(\"y\")\n",
    "    axes[2].set_xlabel(\"x\")\n",
    "    axes[2].set_title(f\"{title}: Predictive Mean + Epistemic Uncertainty\")\n",
    "    axes[2].legend(loc=\"upper left\")\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc0f18",
   "metadata": {},
   "source": [
    "## Sample size Heteroscedastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different function types: linear, non-linear, sin\n",
    "function_types = [\"linear\", \"non-linear\"]\n",
    "function_names = {\"linear\": \"Linear\", \"non-linear\": \"Non-Linear\"}\n",
    "\n",
    "for func_type in function_types:\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# Function Type: {function_names[func_type]} ({func_type})\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "    \n",
    "    # Loop through different training data amounts: 1%, 5%, 10%, 15%, 50%, 100%\n",
    "    # Generate full training dataset using generate_toy_regression function\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Generate training dataset\n",
    "    x_train_full, y_train_full, x_grid, y_grid_clean = generate_toy_regression(\n",
    "        n_train=1000, \n",
    "        train_range=(10, 30), \n",
    "        grid_points=600,\n",
    "        noise_type='heteroscedastic',\n",
    "        type=func_type\n",
    "    )\n",
    "\n",
    "    # Training data percentages to test\n",
    "    percentages = [1, 5, 10, 15, 25, 50, 100]\n",
    "      \n",
    "    # Main loop: train and evaluate for each percentage\n",
    "    n_train_full = len(x_train_full)\n",
    "\n",
    "    # Store uncertainties for each percentage\n",
    "    uncertainties_by_pct = {pct: {'ale': [], 'epi': [], 'tot': []} for pct in percentages}\n",
    "\n",
    "    for pct in percentages:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training with {pct}% of training data ({int(n_train_full * pct / 100)} samples)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Set seed for reproducibility of subsampling and training\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Subsample training data\n",
    "        n_samples = int(n_train_full * pct / 100)\n",
    "        indices = np.random.choice(n_train_full, size=n_samples, replace=False)\n",
    "        x_train_subset = x_train_full[indices]\n",
    "        y_train_subset = y_train_full[indices]\n",
    "        \n",
    "        # Normalize input x (y stays in original scale)\n",
    "        x_mean, x_std = normalize_x(x_train_subset)\n",
    "        x_train_subset_norm = normalize_x_data(x_train_subset, x_mean, x_std)\n",
    "        x_grid_norm = normalize_x_data(x_grid, x_mean, x_std)\n",
    "        \n",
    "        # Create dataloader with normalized x (y stays in original scale)\n",
    "        ds = TensorDataset(torch.from_numpy(x_train_subset_norm), torch.from_numpy(y_train_subset))\n",
    "        loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Train MC Dropout model with beta-NLL (β=0.5)\n",
    "        model = MCDropoutRegressor(p=0.1)\n",
    "        train_model(model, loader, epochs=700, lr=1e-3, loss_type='beta_nll', beta=0.5)\n",
    "        \n",
    "        # Make predictions using normalized x_grid (predictions are in original y scale)\n",
    "        mu_pred, ale_var, epi_var, tot_var = mc_dropout_predict(model, x_grid_norm, M=20)\n",
    "        \n",
    "        # Store uncertainties (using variance values)\n",
    "        uncertainties_by_pct[pct]['ale'].append(ale_var)\n",
    "        uncertainties_by_pct[pct]['epi'].append(epi_var)\n",
    "        uncertainties_by_pct[pct]['tot'].append(tot_var)\n",
    "        \n",
    "        # Plot uncertainties (original absolute units plot)\n",
    "        plot_uncertainties_no_ood(x_train_subset, y_train_subset, x_grid, y_grid_clean, \n",
    "                                 mu_pred, ale_var, epi_var, tot_var, \n",
    "                                 title=f\"MC Dropout (β-NLL, β=0.5) - {function_names[func_type]} - {pct}% training data\")\n",
    "    \n",
    "\n",
    "    # Normalize and compute averages for each percentage\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Normalized Average Uncertainties by Percentage - {function_names[func_type]} Function\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # First, collect all values to compute global min/max for normalization\n",
    "    all_ale = np.concatenate([np.concatenate(uncertainties_by_pct[pct]['ale']) for pct in percentages])\n",
    "    all_epi = np.concatenate([np.concatenate(uncertainties_by_pct[pct]['epi']) for pct in percentages])\n",
    "    \n",
    "\n",
    "    # Compute min and max for normalization\n",
    "    ale_min, ale_max = all_ale.min(), all_ale.max()\n",
    "    epi_min, epi_max = all_epi.min(), all_epi.max()\n",
    "    \n",
    "\n",
    "\n",
    "    # Normalize function\n",
    "    def normalize(values, vmin, vmax):\n",
    "        \"\"\"Normalize values to [0, 1] range\"\"\"\n",
    "        if vmax - vmin == 0:\n",
    "            return np.zeros_like(values)\n",
    "        return (values - vmin) / (vmax - vmin)\n",
    "\n",
    "    # Compute and print normalized averages and correlations for each percentage\n",
    "    print(f\"\\n{'Percentage':<12} {'Avg Aleatoric (norm)':<25} {'Avg Epistemic (norm)':<25} {'Avg Total (norm)':<25} {'Correlation (Epi-Ale)':<25}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    # Store averages and correlations for plotting\n",
    "    avg_ale_norm_list = []\n",
    "    avg_epi_norm_list = []\n",
    "    avg_tot_norm_list = []\n",
    "    correlation_list = []\n",
    "\n",
    "    for pct in percentages:\n",
    "        # Get uncertainties for this percentage\n",
    "        ale_vals = np.concatenate(uncertainties_by_pct[pct]['ale'])\n",
    "        epi_vals = np.concatenate(uncertainties_by_pct[pct]['epi'])\n",
    "        tot_vals = np.concatenate(uncertainties_by_pct[pct]['tot'])\n",
    "        \n",
    "        # Normalize\n",
    "        ale_norm = normalize(ale_vals, ale_min, ale_max)\n",
    "        epi_norm = normalize(epi_vals, epi_min, epi_max)\n",
    "        tot_norm = ale_norm + epi_norm\n",
    "        \n",
    "        # Compute averages\n",
    "        avg_ale_norm = np.mean(ale_norm)\n",
    "        avg_epi_norm = np.mean(epi_norm)\n",
    "        avg_tot_norm = np.mean(tot_norm)\n",
    "        \n",
    "        # Compute correlation between epistemic and aleatoric uncertainties\n",
    "        # Use original (non-normalized) values for correlation\n",
    "        correlation = np.corrcoef(epi_vals, ale_vals)[0, 1]\n",
    "        if np.isnan(correlation):\n",
    "            correlation = 0.0  # Handle case where std is zero\n",
    "        \n",
    "        # Store for plotting\n",
    "        avg_ale_norm_list.append(avg_ale_norm)\n",
    "        avg_epi_norm_list.append(avg_epi_norm)\n",
    "        avg_tot_norm_list.append(avg_tot_norm)\n",
    "        correlation_list.append(correlation)\n",
    "        \n",
    "        print(f\"{pct:>3}%        {avg_ale_norm:>24.6f}  {avg_epi_norm:>24.6f}  {avg_tot_norm:>24.6f}  {correlation:>24.6f}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Note: Average values are normalized to [0, 1] range across all percentages\")\n",
    "    print(\"      Correlation is computed on original (non-normalized) uncertainty values\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Create line plot of normalized average uncertainties\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(percentages, avg_ale_norm_list, 'o-', linewidth=2, markersize=8, label='Aleatoric Uncertainty', color='green')\n",
    "    plt.plot(percentages, avg_epi_norm_list, 's-', linewidth=2, markersize=8, label='Epistemic Uncertainty', color='orange')\n",
    "    plt.plot(percentages, avg_tot_norm_list, '^-', linewidth=2, markersize=8, label='Total Uncertainty', color='blue')\n",
    "\n",
    "    plt.xlabel('Training Data Percentage (%)', fontsize=12)\n",
    "    plt.ylabel('Normalized Average Uncertainty', fontsize=12)\n",
    "    plt.title(f'Normalized Average Uncertainties vs Training Data Percentage - {function_names[func_type]} Function (Heteroscedastic)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11, loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, 105)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0ae81",
   "metadata": {},
   "source": [
    "## Smaple size Homoscedastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332362ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different function types: linear, non-linear, sin\n",
    "function_types = [\"linear\", \"non-linear\", \"sin\"]\n",
    "function_names = {\"linear\": \"Linear\", \"non-linear\": \"Polynomial\", \"sin\": \"Sinusoidal\"}\n",
    "\n",
    "for func_type in function_types:\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# Function Type: {function_names[func_type]} ({func_type})\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "    \n",
    "    # Loop through different training data amounts: 1%, 5%, 10%, 15%, 50%, 100%\n",
    "    # Generate full training dataset using generate_toy_regression function\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Generate training dataset\n",
    "    x_train_full, y_train_full, x_grid, y_grid_clean = generate_toy_regression(\n",
    "        n_train=1000, \n",
    "        train_range=(10, 30), \n",
    "        grid_points=600,\n",
    "        noise_type='homoscedastic',\n",
    "        type=func_type\n",
    "    )\n",
    "\n",
    "    # Training data percentages to test\n",
    "    percentages = [1, 5, 10, 15, 25, 50, 100]\n",
    "      \n",
    "    # Main loop: train and evaluate for each percentage\n",
    "    n_train_full = len(x_train_full)\n",
    "\n",
    "    # Store uncertainties for each percentage\n",
    "    uncertainties_by_pct = {pct: {'ale': [], 'epi': [], 'tot': []} for pct in percentages}\n",
    "\n",
    "    for pct in percentages:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training with {pct}% of training data ({int(n_train_full * pct / 100)} samples)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Set seed for reproducibility of subsampling and training\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Subsample training data\n",
    "        n_samples = int(n_train_full * pct / 100)\n",
    "        indices = np.random.choice(n_train_full, size=n_samples, replace=False)\n",
    "        x_train_subset = x_train_full[indices]\n",
    "        y_train_subset = y_train_full[indices]\n",
    "        \n",
    "        # Create dataloader\n",
    "        ds = TensorDataset(torch.from_numpy(x_train_subset), torch.from_numpy(y_train_subset))\n",
    "        loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Train MC Dropout model with beta-NLL (β=0.5)\n",
    "        model = MCDropoutRegressor(p=0.1)\n",
    "        train_model(model, loader, epochs=700, lr=1e-3, loss_type='beta_nll', beta=0.5)\n",
    "        \n",
    "        # Make predictions\n",
    "        mu_pred, ale_var, epi_var, tot_var = mc_dropout_predict(model, x_grid, M=20)\n",
    "        \n",
    "        # Store uncertainties (using variance values)\n",
    "        uncertainties_by_pct[pct]['ale'].append(ale_var)\n",
    "        uncertainties_by_pct[pct]['epi'].append(epi_var)\n",
    "        uncertainties_by_pct[pct]['tot'].append(tot_var)\n",
    "    \n",
    "        # Plot uncertainties\n",
    "        plot_uncertainties_no_ood(x_train_subset, y_train_subset, x_grid, y_grid_clean, \n",
    "                                 mu_pred, ale_var, epi_var, tot_var, \n",
    "                                 title=f\"MC Dropout (β-NLL, β=0.5) - {function_names[func_type]} - {pct}% training data\")\n",
    "\n",
    "    # Normalize and compute averages for each percentage\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Normalized Average Uncertainties by Percentage - {function_names[func_type]} Function\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # First, collect all values to compute global min/max for normalization\n",
    "    all_ale = np.concatenate([np.concatenate(uncertainties_by_pct[pct]['ale']) for pct in percentages])\n",
    "    all_epi = np.concatenate([np.concatenate(uncertainties_by_pct[pct]['epi']) for pct in percentages])\n",
    "    all_tot = np.concatenate([np.concatenate(uncertainties_by_pct[pct]['tot']) for pct in percentages])\n",
    "\n",
    "    # Compute min and max for normalization\n",
    "    ale_min, ale_max = all_ale.min(), all_ale.max()\n",
    "    epi_min, epi_max = all_epi.min(), all_epi.max()\n",
    "    tot_min, tot_max = all_tot.min(), all_tot.max()\n",
    "\n",
    "    # Normalize function\n",
    "    def normalize(values, vmin, vmax):\n",
    "        \"\"\"Normalize values to [0, 1] range\"\"\"\n",
    "        if vmax - vmin == 0:\n",
    "            return np.zeros_like(values)\n",
    "        return (values - vmin) / (vmax - vmin)\n",
    "\n",
    "    # Compute and print normalized averages and correlations for each percentage\n",
    "    print(f\"\\n{'Percentage':<12} {'Avg Aleatoric (norm)':<25} {'Avg Epistemic (norm)':<25} {'Avg Total (norm)':<25} {'Correlation (Epi-Ale)':<25}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    # Store averages and correlations for plotting\n",
    "    avg_ale_norm_list = []\n",
    "    avg_epi_norm_list = []\n",
    "    avg_tot_norm_list = []\n",
    "    correlation_list = []\n",
    "\n",
    "    for pct in percentages:\n",
    "        # Get uncertainties for this percentage\n",
    "        ale_vals = np.concatenate(uncertainties_by_pct[pct]['ale'])\n",
    "        epi_vals = np.concatenate(uncertainties_by_pct[pct]['epi'])\n",
    "        tot_vals = np.concatenate(uncertainties_by_pct[pct]['tot'])\n",
    "        \n",
    "        # Normalize\n",
    "        ale_norm = normalize(ale_vals, ale_min, ale_max)\n",
    "        epi_norm = normalize(epi_vals, epi_min, epi_max)\n",
    "        tot_norm = ale_norm + epi_norm\n",
    "        \n",
    "        # Compute averages\n",
    "        avg_ale_norm = np.mean(ale_norm)\n",
    "        avg_epi_norm = np.mean(epi_norm)\n",
    "        avg_tot_norm = np.mean(tot_norm)\n",
    "        \n",
    "        # Compute correlation between epistemic and aleatoric uncertainties\n",
    "        # Use original (non-normalized) values for correlation\n",
    "        correlation = np.corrcoef(epi_vals, ale_vals)[0, 1]\n",
    "        if np.isnan(correlation):\n",
    "            correlation = 0.0  # Handle case where std is zero\n",
    "        \n",
    "        # Store for plotting\n",
    "        avg_ale_norm_list.append(avg_ale_norm)\n",
    "        avg_epi_norm_list.append(avg_epi_norm)\n",
    "        avg_tot_norm_list.append(avg_tot_norm)\n",
    "        correlation_list.append(correlation)\n",
    "    \n",
    "        print(f\"{pct:>3}%        {avg_ale_norm:>24.6f}  {avg_epi_norm:>24.6f}  {avg_tot_norm:>24.6f}  {correlation:>24.6f}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Note: Average values are normalized to [0, 1] range across all percentages\")\n",
    "    print(\"      Correlation is computed on original (non-normalized) uncertainty values\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Create line plot of normalized average uncertainties\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(percentages, avg_ale_norm_list, 'o-', linewidth=2, markersize=8, label='Aleatoric Uncertainty', color='green')\n",
    "    plt.plot(percentages, avg_epi_norm_list, 's-', linewidth=2, markersize=8, label='Epistemic Uncertainty', color='orange')\n",
    "    plt.plot(percentages, avg_tot_norm_list, '^-', linewidth=2, markersize=8, label='Total Uncertainty', color='blue')\n",
    "\n",
    "    plt.xlabel('Training Data Percentage (%)', fontsize=12)\n",
    "    plt.ylabel('Normalized Average Uncertainty', fontsize=12)\n",
    "    plt.title(f'Normalized Average Uncertainties vs Training Data Percentage - {function_names[func_type]} Function (Homoscedastic)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11, loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, 105)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a51cf2",
   "metadata": {},
   "source": [
    "### Esemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6964247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----- Run end-to-end -----\n",
    "def main():\n",
    "    # Data - 1000 samples\n",
    "    x_train, y_train, x_grid, y_clean = generate_toy_regression(\n",
    "        n_train=1000,\n",
    "        train_range=(0.0, 10.0),\n",
    "        noise_type='heteroscedastic'\n",
    "    )\n",
    "\n",
    "    # Train ensemble with NLL\n",
    "    ens_nll = train_ensemble(x_train, y_train, batch_size=32, K=5, loss_type='nll', beta=0.5)\n",
    "    mu_nll, ale_nll, epi_nll, tot_nll = ensemble_predict(ens_nll, x_grid)\n",
    "    plot_uncertainties_no_ood(x_train, y_train, x_grid, y_clean, mu_nll, ale_nll, epi_nll, tot_nll, title=\"Ensembles (NLL)\")\n",
    "\n",
    "    # Train ensemble with β-NLL (β=0.5)\n",
    "    ens_bnll = train_ensemble(x_train, y_train, batch_size=32, K=5, loss_type='beta_nll', beta=0.5)\n",
    "    mu_bnll, ale_bnll, epi_bnll, tot_bnll = ensemble_predict(ens_bnll, x_grid)\n",
    "    plot_uncertainties_no_ood(x_train, y_train, x_grid, y_clean, mu_bnll, ale_bnll, epi_bnll, tot_bnll, title=\"Ensembles (β-NLL, β=0.5)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
