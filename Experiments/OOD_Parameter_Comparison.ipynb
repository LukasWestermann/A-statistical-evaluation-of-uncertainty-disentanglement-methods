{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\n",
            "Trying to import in ABI mode.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results will be saved to: c:\\Users\\lukas\\OneDrive\\Desktop\\Code-Masterarbeit\\A-statistical-evaluation-of-uncertainty-disentanglement-methods-1\\results\\ood_parameter_comparison\n",
            "CUDA not available. Using CPU.\n",
            "CUDA not available. Using CPU.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from typing import Tuple\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pyro\n",
        "\n",
        "# Add parent directory to path to import Models\n",
        "# This works for notebooks in the Experiments folder\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'Experiments' else Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Setup results directory\n",
        "results_dir = project_root / \"results\" / \"ood_parameter_comparison\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "plots_dir = results_dir / \"plots\"\n",
        "plots_dir.mkdir(exist_ok=True)\n",
        "stats_dir = results_dir / \"statistics\"\n",
        "stats_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Results will be saved to: {results_dir}\")\n",
        "\n",
        "# Import from Models folder\n",
        "from Models.MC_Dropout import (\n",
        "    MCDropoutRegressor,\n",
        "    train_model,\n",
        "    mc_dropout_predict,\n",
        "    gaussian_nll,\n",
        "    beta_nll,\n",
        "    plot_toy_data,\n",
        "    plot_uncertainties,\n",
        "    normalize_x,\n",
        "    normalize_x_data\n",
        ")\n",
        "\n",
        "from Models.Deep_Ensemble import (\n",
        "    train_ensemble_deep,\n",
        "    ensemble_predict_deep\n",
        ")\n",
        "\n",
        "from utils.device import get_device\n",
        "from utils.plotting import plot_toy_data, plot_uncertainties_ood\n",
        "import utils.results_save as results_save_module\n",
        "from utils.results_save import save_plot, save_statistics\n",
        "\n",
        "# Import OOD helper functions\n",
        "from utils.ood_experiments import (\n",
        "    generate_data_with_ood,\n",
        "    compute_and_save_statistics_ood\n",
        ")\n",
        "\n",
        "# Set the module-level directories for results_save\n",
        "results_save_module.plots_dir = plots_dir\n",
        "results_save_module.stats_dir = stats_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Device Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = get_device()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Toy Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# ----- Data generation for linear function with homo/heteroscedastic noise -----\n",
        "def generate_toy_regression(n_train=1000, train_range=(0.0, 10.0), train_ranges=None,\n",
        "                           ood_ranges=None, grid_points=1000, noise_type='heteroscedastic', type = \"linear\"):\n",
        "    \"\"\"\n",
        "    Generate toy regression data with support for multiple training ranges and OOD regions.\n",
        "    \n",
        "    Args:\n",
        "        n_train: Number of training samples\n",
        "        train_range: Single training range tuple (min, max) - for backward compatibility\n",
        "        train_ranges: List of training range tuples [(min1, max1), (min2, max2), ...]\n",
        "                     If provided, overrides train_range. Samples are distributed proportionally.\n",
        "        ood_ranges: List of OOD range tuples [(min1, max1), (min2, max2), ...]\n",
        "                   If None, OOD is automatically everything NOT in training ranges\n",
        "        grid_points: Number of grid points for evaluation\n",
        "        noise_type: 'homoscedastic' or 'heteroscedastic'\n",
        "        type: 'linear' or 'sin'\n",
        "    \n",
        "    Returns:\n",
        "        (x_train, y_train, x_grid, y_grid_clean, ood_mask)\n",
        "    \"\"\"\n",
        "    # Handle train_ranges: if provided, use it; otherwise use train_range as single range\n",
        "    if train_ranges is None:\n",
        "        train_ranges = [train_range]\n",
        "    else:\n",
        "        # train_ranges provided, ignore train_range\n",
        "        pass\n",
        "    \n",
        "    # Sample training data proportionally from each training range\n",
        "    # Calculate total width of all training ranges\n",
        "    total_width = sum([r[1] - r[0] for r in train_ranges])\n",
        "    \n",
        "    # Sample from each range proportionally\n",
        "    x_train_list = []\n",
        "    samples_allocated = 0\n",
        "    for idx, train_r in enumerate(train_ranges):\n",
        "        low, high = train_r\n",
        "        range_width = high - low\n",
        "        # Number of samples proportional to range width\n",
        "        if idx == len(train_ranges) - 1:\n",
        "            # Last range gets remaining samples to ensure exact total\n",
        "            n_samples = n_train - samples_allocated\n",
        "        else:\n",
        "            n_samples = int(n_train * range_width / total_width)\n",
        "            samples_allocated += n_samples\n",
        "        x_train_range = np.random.uniform(low, high, size=(n_samples, 1))\n",
        "        x_train_list.append(x_train_range)\n",
        "    \n",
        "    x_train = np.vstack(x_train_list)\n",
        "    # Shuffle to mix samples from different ranges\n",
        "    indices = np.random.permutation(len(x_train))\n",
        "    x_train = x_train[indices]\n",
        "    \n",
        "    if type == \"linear\":\n",
        "        # Linear function: f(x) = 0.7x + 0.5\n",
        "        f_clean = lambda x: 0.7 * x + 0.5\n",
        "        y_clean_train = f_clean(x_train)\n",
        "    elif type == \"sin\":\n",
        "        f_clean = lambda x:  x * np.sin(x) + x\n",
        "        y_clean_train = f_clean(x_train)\n",
        "    else:\n",
        "        raise ValueError(\"type must be 'linear', 'sin'\")\n",
        "\n",
        "    # Define noise variance σ²(x)\n",
        "    if noise_type == 'homoscedastic':\n",
        "        # Homoscedastic: σ(x) = 0.8\n",
        "        sigma = 2\n",
        "        sigma_train = np.full_like(x_train, sigma)\n",
        "    elif noise_type == 'heteroscedastic':\n",
        "        # Heteroscedastic: \n",
        "        sigma_train = np.abs(2.5 * np.sin(0.5*x_train +5))\n",
        "    else:\n",
        "        raise ValueError(\"noise_type must be 'homoscedastic' or 'heteroscedastic'\")\n",
        "    \n",
        "    # Generate noise: ε | x ~ N(0, σ²(x))\n",
        "    epsilon = np.random.normal(0.0, sigma_train, size=(n_train, 1))\n",
        "    y_train = y_clean_train + epsilon\n",
        "\n",
        "    # Determine grid extent: from min of all training/OOD ranges to max\n",
        "    all_ranges = train_ranges + (ood_ranges if ood_ranges else [])\n",
        "    grid_start = min([r[0] for r in all_ranges])\n",
        "    grid_end = max([r[1] for r in all_ranges])\n",
        "    \n",
        "    # Dense evaluation grid spanning all training and OOD regions\n",
        "    x_grid = np.linspace(grid_start, grid_end, grid_points).reshape(-1, 1)\n",
        "    y_grid_clean = f_clean(x_grid)\n",
        "    \n",
        "    # Create OOD mask: True for points NOT in any training range\n",
        "    # Everything outside training ranges is OOD (including gaps and explicit OOD ranges)\n",
        "    ood_mask = np.ones(len(x_grid), dtype=bool)  # Start with all True (OOD)\n",
        "    \n",
        "    # Mark training ranges as ID (False in ood_mask)\n",
        "    for train_r in train_ranges:\n",
        "        train_start, train_end = train_r\n",
        "        train_mask = (x_grid[:, 0] >= train_start) & (x_grid[:, 0] <= train_end)\n",
        "        ood_mask[train_mask] = False  # Training regions are ID, not OOD\n",
        "    \n",
        "    # If explicit ood_ranges provided, ensure they are marked as OOD\n",
        "    # (they might already be OOD if they're gaps, but this ensures they're marked)\n",
        "    if ood_ranges is not None:\n",
        "        for ood_range in ood_ranges:\n",
        "            ood_start, ood_end = ood_range\n",
        "            ood_mask |= (x_grid[:, 0] >= ood_start) & (x_grid[:, 0] <= ood_end)\n",
        "\n",
        "    return (x_train.astype(np.float32), y_train.astype(np.float32),\n",
        "            x_grid.astype(np.float32), y_grid_clean.astype(np.float32), ood_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common parameters\n",
        "n_train = 1000\n",
        "train_range = (-5, 10)\n",
        "ood_ranges = [(30, 40)]  # List of (min, max) tuples for OOD regions\n",
        "grid_points = 1000\n",
        "seed = 42\n",
        "noise_type = 'heteroscedastic'\n",
        "func_type = 'sin'  # or 'linear'\n",
        "function_name = \"Sinusoidal\" if func_type == 'sin' else \"Linear\"\n",
        "\n",
        "# Parameters to vary\n",
        "mc_samples_values = [10, 20, 50, 100]  # MC Dropout forward passes\n",
        "dropout_p_values = [0.2]  # Can add more: [0.1, 0.2, 0.3]\n",
        "K_values = [3, 5, 10, 20]  # Deep Ensemble number of nets\n",
        "\n",
        "# Fixed training parameters\n",
        "beta = 0.5\n",
        "epochs = 700\n",
        "lr = 1e-3\n",
        "batch_size = 32\n",
        "\n",
        "torch.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions for Parameter Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_single_mc_dropout_ood(generate_toy_regression_func, x_train, y_train, x_grid, y_grid_clean, ood_mask,\n",
        "                              p, mc_samples, beta, epochs, lr, batch_size, seed, \n",
        "                              function_name, noise_type, func_type, date, save_results=True):\n",
        "    \"\"\"Run a single MC Dropout OOD experiment and return results\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    model = MCDropoutRegressor(p=p)\n",
        "    train_model(model, loader, epochs=epochs, lr=lr, loss_type='beta_nll', beta=beta)\n",
        "    \n",
        "    # Make predictions\n",
        "    mu_pred, ale_var, epi_var, tot_var = mc_dropout_predict(model, x_grid, M=mc_samples)\n",
        "    \n",
        "    # Split uncertainties by region\n",
        "    id_mask = ~ood_mask\n",
        "    \n",
        "    uncertainties_id = {\n",
        "        'ale': ale_var[id_mask] if ale_var.ndim == 1 else ale_var[id_mask].flatten(),\n",
        "        'epi': epi_var[id_mask] if epi_var.ndim == 1 else epi_var[id_mask].flatten(),\n",
        "        'tot': tot_var[id_mask] if tot_var.ndim == 1 else tot_var[id_mask].flatten()\n",
        "    }\n",
        "    \n",
        "    uncertainties_ood = {\n",
        "        'ale': ale_var[ood_mask] if ale_var.ndim == 1 else ale_var[ood_mask].flatten(),\n",
        "        'epi': epi_var[ood_mask] if epi_var.ndim == 1 else epi_var[ood_mask].flatten(),\n",
        "        'tot': tot_var[ood_mask] if tot_var.ndim == 1 else tot_var[ood_mask].flatten()\n",
        "    }\n",
        "    \n",
        "    uncertainties_combined = {\n",
        "        'ale': ale_var.flatten() if ale_var.ndim > 1 else ale_var,\n",
        "        'epi': epi_var.flatten() if epi_var.ndim > 1 else epi_var,\n",
        "        'tot': tot_var.flatten() if tot_var.ndim > 1 else tot_var\n",
        "    }\n",
        "    \n",
        "    # Compute MSE separately\n",
        "    mu_pred_flat = mu_pred.squeeze() if mu_pred.ndim > 1 else mu_pred\n",
        "    y_grid_clean_flat = y_grid_clean.squeeze() if y_grid_clean.ndim > 1 else y_grid_clean\n",
        "    \n",
        "    mse_id = np.mean((mu_pred_flat[id_mask] - y_grid_clean_flat[id_mask])**2)\n",
        "    mse_ood = np.mean((mu_pred_flat[ood_mask] - y_grid_clean_flat[ood_mask])**2)\n",
        "    mse_combined = np.mean((mu_pred_flat - y_grid_clean_flat)**2)\n",
        "    \n",
        "    # Save statistics if requested\n",
        "    if save_results:\n",
        "        compute_and_save_statistics_ood(\n",
        "            uncertainties_id, uncertainties_ood, uncertainties_combined,\n",
        "            mse_id, mse_ood, mse_combined,\n",
        "            function_name, noise_type, func_type, 'MC_Dropout',\n",
        "            date=date, dropout_p=p, mc_samples=mc_samples\n",
        "        )\n",
        "    \n",
        "    return {\n",
        "        'uncertainties_id': uncertainties_id,\n",
        "        'uncertainties_ood': uncertainties_ood,\n",
        "        'uncertainties_combined': uncertainties_combined,\n",
        "        'mse_id': mse_id,\n",
        "        'mse_ood': mse_ood,\n",
        "        'mse_combined': mse_combined,\n",
        "        'mu_pred': mu_pred,\n",
        "        'ale_var': ale_var,\n",
        "        'epi_var': epi_var,\n",
        "        'tot_var': tot_var\n",
        "    }\n",
        "\n",
        "\n",
        "def run_single_deep_ensemble_ood(generate_toy_regression_func, x_train, y_train, x_grid, y_grid_clean, ood_mask,\n",
        "                                 K, beta, batch_size, seed,\n",
        "                                 function_name, noise_type, func_type, date, save_results=True):\n",
        "    \"\"\"Run a single Deep Ensemble OOD experiment and return results\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    x_mean, x_std = normalize_x(x_train)\n",
        "    x_train_norm = normalize_x_data(x_train, x_mean, x_std)\n",
        "    x_grid_norm = normalize_x_data(x_grid, x_mean, x_std)\n",
        "    \n",
        "    ensemble = train_ensemble_deep(\n",
        "        x_train_norm, y_train,\n",
        "        batch_size=batch_size, K=K,\n",
        "        loss_type='beta_nll', beta=beta, parallel=True\n",
        "    )\n",
        "    \n",
        "    # Make predictions\n",
        "    mu_pred, ale_var, epi_var, tot_var = ensemble_predict_deep(ensemble, x_grid_norm)\n",
        "    \n",
        "    # Split uncertainties by region\n",
        "    id_mask = ~ood_mask\n",
        "    \n",
        "    uncertainties_id = {\n",
        "        'ale': ale_var[id_mask] if ale_var.ndim == 1 else ale_var[id_mask].flatten(),\n",
        "        'epi': epi_var[id_mask] if epi_var.ndim == 1 else epi_var[id_mask].flatten(),\n",
        "        'tot': tot_var[id_mask] if tot_var.ndim == 1 else tot_var[id_mask].flatten()\n",
        "    }\n",
        "    \n",
        "    uncertainties_ood = {\n",
        "        'ale': ale_var[ood_mask] if ale_var.ndim == 1 else ale_var[ood_mask].flatten(),\n",
        "        'epi': epi_var[ood_mask] if epi_var.ndim == 1 else epi_var[ood_mask].flatten(),\n",
        "        'tot': tot_var[ood_mask] if tot_var.ndim == 1 else tot_var[ood_mask].flatten()\n",
        "    }\n",
        "    \n",
        "    uncertainties_combined = {\n",
        "        'ale': ale_var.flatten() if ale_var.ndim > 1 else ale_var,\n",
        "        'epi': epi_var.flatten() if epi_var.ndim > 1 else epi_var,\n",
        "        'tot': tot_var.flatten() if tot_var.ndim > 1 else tot_var\n",
        "    }\n",
        "    \n",
        "    # Compute MSE separately\n",
        "    mu_pred_flat = mu_pred.squeeze() if mu_pred.ndim > 1 else mu_pred\n",
        "    y_grid_clean_flat = y_grid_clean.squeeze() if y_grid_clean.ndim > 1 else y_grid_clean\n",
        "    \n",
        "    mse_id = np.mean((mu_pred_flat[id_mask] - y_grid_clean_flat[id_mask])**2)\n",
        "    mse_ood = np.mean((mu_pred_flat[ood_mask] - y_grid_clean_flat[ood_mask])**2)\n",
        "    mse_combined = np.mean((mu_pred_flat - y_grid_clean_flat)**2)\n",
        "    \n",
        "    # Save statistics if requested\n",
        "    if save_results:\n",
        "        compute_and_save_statistics_ood(\n",
        "            uncertainties_id, uncertainties_ood, uncertainties_combined,\n",
        "            mse_id, mse_ood, mse_combined,\n",
        "            function_name, noise_type, func_type, 'Deep_Ensemble',\n",
        "            date=date, n_nets=K\n",
        "        )\n",
        "    \n",
        "    return {\n",
        "        'uncertainties_id': uncertainties_id,\n",
        "        'uncertainties_ood': uncertainties_ood,\n",
        "        'uncertainties_combined': uncertainties_combined,\n",
        "        'mse_id': mse_id,\n",
        "        'mse_ood': mse_ood,\n",
        "        'mse_combined': mse_combined,\n",
        "        'mu_pred': mu_pred,\n",
        "        'ale_var': ale_var,\n",
        "        'epi_var': epi_var,\n",
        "        'tot_var': tot_var\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Data (Once)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data once (same for all parameter variations)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "x_train, y_train, x_grid, y_grid_clean, ood_mask = generate_data_with_ood(\n",
        "    generate_toy_regression, n_train, train_range, ood_ranges,\n",
        "    grid_points, noise_type, func_type, seed\n",
        ")\n",
        "\n",
        "print(f\"Training range: {train_range}\")\n",
        "print(f\"OOD ranges: {ood_ranges}\")\n",
        "print(f\"Grid spans: [{x_grid[0, 0]:.2f}, {x_grid[-1, 0]:.2f}]\")\n",
        "print(f\"ID points: {np.sum(~ood_mask)}, OOD points: {np.sum(ood_mask)}\")\n",
        "print(f\"Function type: {function_name} ({func_type})\")\n",
        "print(f\"Noise type: {noise_type}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MC Dropout - Vary mc_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate date for this experiment batch\n",
        "date = datetime.now().strftime('%Y%m%d')\n",
        "\n",
        "# Store results for comparison\n",
        "results_mc_dropout = {}\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"MC Dropout Parameter Comparison - Varying mc_samples\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "for p in dropout_p_values:\n",
        "    for mc_samples in mc_samples_values:\n",
        "        param_key = f\"p{p}_M{mc_samples}\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Testing: p={p}, mc_samples={mc_samples}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        result = run_single_mc_dropout_ood(\n",
        "            generate_toy_regression, x_train, y_train, x_grid, y_grid_clean, ood_mask,\n",
        "            p=p, mc_samples=mc_samples, beta=beta, epochs=epochs, lr=lr, batch_size=batch_size,\n",
        "            seed=seed, function_name=function_name, noise_type=noise_type, func_type=func_type,\n",
        "            date=date, save_results=True\n",
        "        )\n",
        "        \n",
        "        results_mc_dropout[param_key] = result\n",
        "        \n",
        "        # Print summary\n",
        "        print(f\"  ID - Avg Ale: {np.mean(result['uncertainties_id']['ale']):.6f}, \"\n",
        "              f\"Avg Epi: {np.mean(result['uncertainties_id']['epi']):.6f}, \"\n",
        "              f\"MSE: {result['mse_id']:.6f}\")\n",
        "        print(f\"  OOD - Avg Ale: {np.mean(result['uncertainties_ood']['ale']):.6f}, \"\n",
        "              f\"Avg Epi: {np.mean(result['uncertainties_ood']['epi']):.6f}, \"\n",
        "              f\"MSE: {result['mse_ood']:.6f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"MC Dropout experiments completed!\")\n",
        "print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MC Dropout - Comparison Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract data for plotting\n",
        "mc_samples_list = []\n",
        "avg_ale_id_list = []\n",
        "avg_epi_id_list = []\n",
        "avg_tot_id_list = []\n",
        "avg_ale_ood_list = []\n",
        "avg_epi_ood_list = []\n",
        "avg_tot_ood_list = []\n",
        "mse_id_list = []\n",
        "mse_ood_list = []\n",
        "\n",
        "for param_key, result in results_mc_dropout.items():\n",
        "    # Extract mc_samples from param_key (format: \"p0.2_M20\")\n",
        "    mc_samples_val = int(param_key.split('_M')[1])\n",
        "    mc_samples_list.append(mc_samples_val)\n",
        "    \n",
        "    avg_ale_id_list.append(np.mean(result['uncertainties_id']['ale']))\n",
        "    avg_epi_id_list.append(np.mean(result['uncertainties_id']['epi']))\n",
        "    avg_tot_id_list.append(np.mean(result['uncertainties_id']['tot']))\n",
        "    \n",
        "    avg_ale_ood_list.append(np.mean(result['uncertainties_ood']['ale']))\n",
        "    avg_epi_ood_list.append(np.mean(result['uncertainties_ood']['epi']))\n",
        "    avg_tot_ood_list.append(np.mean(result['uncertainties_ood']['tot']))\n",
        "    \n",
        "    mse_id_list.append(result['mse_id'])\n",
        "    mse_ood_list.append(result['mse_ood'])\n",
        "\n",
        "# Sort by mc_samples\n",
        "sorted_indices = np.argsort(mc_samples_list)\n",
        "mc_samples_list = [mc_samples_list[i] for i in sorted_indices]\n",
        "avg_ale_id_list = [avg_ale_id_list[i] for i in sorted_indices]\n",
        "avg_epi_id_list = [avg_epi_id_list[i] for i in sorted_indices]\n",
        "avg_tot_id_list = [avg_tot_id_list[i] for i in sorted_indices]\n",
        "avg_ale_ood_list = [avg_ale_ood_list[i] for i in sorted_indices]\n",
        "avg_epi_ood_list = [avg_epi_ood_list[i] for i in sorted_indices]\n",
        "avg_tot_ood_list = [avg_tot_ood_list[i] for i in sorted_indices]\n",
        "mse_id_list = [mse_id_list[i] for i in sorted_indices]\n",
        "mse_ood_list = [mse_ood_list[i] for i in sorted_indices]\n",
        "\n",
        "# Create comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Average Uncertainties - ID region\n",
        "axes[0, 0].plot(mc_samples_list, avg_ale_id_list, 'o-', label='Aleatoric (ID)', color='green', linewidth=2, markersize=8)\n",
        "axes[0, 0].plot(mc_samples_list, avg_epi_id_list, 's-', label='Epistemic (ID)', color='orange', linewidth=2, markersize=8)\n",
        "axes[0, 0].plot(mc_samples_list, avg_tot_id_list, '^-', label='Total (ID)', color='blue', linewidth=2, markersize=8)\n",
        "axes[0, 0].set_xlabel('MC Samples', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Average Uncertainty', fontsize=12)\n",
        "axes[0, 0].set_title(f'MC Dropout: Average Uncertainties (ID) vs MC Samples\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].set_xticks(mc_samples_list)\n",
        "\n",
        "# Plot 2: Average Uncertainties - OOD region\n",
        "axes[0, 1].plot(mc_samples_list, avg_ale_ood_list, 'o-', label='Aleatoric (OOD)', color='green', linewidth=2, markersize=8)\n",
        "axes[0, 1].plot(mc_samples_list, avg_epi_ood_list, 's-', label='Epistemic (OOD)', color='orange', linewidth=2, markersize=8)\n",
        "axes[0, 1].plot(mc_samples_list, avg_tot_ood_list, '^-', label='Total (OOD)', color='blue', linewidth=2, markersize=8)\n",
        "axes[0, 1].set_xlabel('MC Samples', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Average Uncertainty', fontsize=12)\n",
        "axes[0, 1].set_title(f'MC Dropout: Average Uncertainties (OOD) vs MC Samples\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].set_xticks(mc_samples_list)\n",
        "\n",
        "# Plot 3: MSE comparison\n",
        "axes[1, 0].plot(mc_samples_list, mse_id_list, 'o-', label='MSE (ID)', color='blue', linewidth=2, markersize=8)\n",
        "axes[1, 0].plot(mc_samples_list, mse_ood_list, 's-', label='MSE (OOD)', color='red', linewidth=2, markersize=8)\n",
        "axes[1, 0].set_xlabel('MC Samples', fontsize=12)\n",
        "axes[1, 0].set_ylabel('MSE', fontsize=12)\n",
        "axes[1, 0].set_title(f'MC Dropout: MSE vs MC Samples\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=10)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_yscale('log')\n",
        "axes[1, 0].set_xticks(mc_samples_list)\n",
        "\n",
        "# Plot 4: ID vs OOD comparison (bar chart for one parameter)\n",
        "x_pos = np.arange(len(mc_samples_list))\n",
        "width = 0.35\n",
        "axes[1, 1].bar(x_pos - width/2, avg_epi_id_list, width, label='Epistemic (ID)', color='orange', alpha=0.7)\n",
        "axes[1, 1].bar(x_pos + width/2, avg_epi_ood_list, width, label='Epistemic (OOD)', color='red', alpha=0.7)\n",
        "axes[1, 1].set_xlabel('MC Samples', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Average Epistemic Uncertainty', fontsize=12)\n",
        "axes[1, 1].set_title(f'MC Dropout: Epistemic Uncertainty - ID vs OOD\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(mc_samples_list)\n",
        "axes[1, 1].legend(fontsize=10)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle(f'MC Dropout Parameter Comparison: Varying MC Samples (p={dropout_p_values[0]})', \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot\n",
        "save_plot(fig, f\"MC_Dropout_mc_samples_comparison_{function_name}_{noise_type}\", \n",
        "          subfolder=f\"comparisons/{noise_type}/{func_type}\")\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "# Create summary table\n",
        "comparison_df = pd.DataFrame({\n",
        "    'MC_Samples': mc_samples_list,\n",
        "    'Avg_Ale_ID': avg_ale_id_list,\n",
        "    'Avg_Epi_ID': avg_epi_id_list,\n",
        "    'Avg_Tot_ID': avg_tot_id_list,\n",
        "    'Avg_Ale_OOD': avg_ale_ood_list,\n",
        "    'Avg_Epi_OOD': avg_epi_ood_list,\n",
        "    'Avg_Tot_OOD': avg_tot_ood_list,\n",
        "    'MSE_ID': mse_id_list,\n",
        "    'MSE_OOD': mse_ood_list\n",
        "})\n",
        "\n",
        "print(\"\\nSummary Table:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Save comparison table\n",
        "save_statistics(comparison_df, f\"MC_Dropout_mc_samples_comparison_{function_name}_{noise_type}\",\n",
        "                subfolder=f\"comparisons/{noise_type}/{func_type}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Ensemble - Vary K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results for comparison\n",
        "results_deep_ensemble = {}\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Deep Ensemble Parameter Comparison - Varying K\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "for K in K_values:\n",
        "    param_key = f\"K{K}\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing: K={K}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    result = run_single_deep_ensemble_ood(\n",
        "        generate_toy_regression, x_train, y_train, x_grid, y_grid_clean, ood_mask,\n",
        "        K=K, beta=beta, batch_size=batch_size, seed=seed,\n",
        "        function_name=function_name, noise_type=noise_type, func_type=func_type,\n",
        "        date=date, save_results=True\n",
        "    )\n",
        "    \n",
        "    results_deep_ensemble[param_key] = result\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"  ID - Avg Ale: {np.mean(result['uncertainties_id']['ale']):.6f}, \"\n",
        "          f\"Avg Epi: {np.mean(result['uncertainties_id']['epi']):.6f}, \"\n",
        "          f\"MSE: {result['mse_id']:.6f}\")\n",
        "    print(f\"  OOD - Avg Ale: {np.mean(result['uncertainties_ood']['ale']):.6f}, \"\n",
        "          f\"Avg Epi: {np.mean(result['uncertainties_ood']['epi']):.6f}, \"\n",
        "          f\"MSE: {result['mse_ood']:.6f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Deep Ensemble experiments completed!\")\n",
        "print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Ensemble - Comparison Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract data for plotting\n",
        "K_list = []\n",
        "avg_ale_id_list_de = []\n",
        "avg_epi_id_list_de = []\n",
        "avg_tot_id_list_de = []\n",
        "avg_ale_ood_list_de = []\n",
        "avg_epi_ood_list_de = []\n",
        "avg_tot_ood_list_de = []\n",
        "mse_id_list_de = []\n",
        "mse_ood_list_de = []\n",
        "\n",
        "for param_key, result in results_deep_ensemble.items():\n",
        "    # Extract K from param_key (format: \"K5\")\n",
        "    K_val = int(param_key[1:])\n",
        "    K_list.append(K_val)\n",
        "    \n",
        "    avg_ale_id_list_de.append(np.mean(result['uncertainties_id']['ale']))\n",
        "    avg_epi_id_list_de.append(np.mean(result['uncertainties_id']['epi']))\n",
        "    avg_tot_id_list_de.append(np.mean(result['uncertainties_id']['tot']))\n",
        "    \n",
        "    avg_ale_ood_list_de.append(np.mean(result['uncertainties_ood']['ale']))\n",
        "    avg_epi_ood_list_de.append(np.mean(result['uncertainties_ood']['epi']))\n",
        "    avg_tot_ood_list_de.append(np.mean(result['uncertainties_ood']['tot']))\n",
        "    \n",
        "    mse_id_list_de.append(result['mse_id'])\n",
        "    mse_ood_list_de.append(result['mse_ood'])\n",
        "\n",
        "# Sort by K\n",
        "sorted_indices = np.argsort(K_list)\n",
        "K_list = [K_list[i] for i in sorted_indices]\n",
        "avg_ale_id_list_de = [avg_ale_id_list_de[i] for i in sorted_indices]\n",
        "avg_epi_id_list_de = [avg_epi_id_list_de[i] for i in sorted_indices]\n",
        "avg_tot_id_list_de = [avg_tot_id_list_de[i] for i in sorted_indices]\n",
        "avg_ale_ood_list_de = [avg_ale_ood_list_de[i] for i in sorted_indices]\n",
        "avg_epi_ood_list_de = [avg_epi_ood_list_de[i] for i in sorted_indices]\n",
        "avg_tot_ood_list_de = [avg_tot_ood_list_de[i] for i in sorted_indices]\n",
        "mse_id_list_de = [mse_id_list_de[i] for i in sorted_indices]\n",
        "mse_ood_list_de = [mse_ood_list_de[i] for i in sorted_indices]\n",
        "\n",
        "# Create comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Average Uncertainties - ID region\n",
        "axes[0, 0].plot(K_list, avg_ale_id_list_de, 'o-', label='Aleatoric (ID)', color='green', linewidth=2, markersize=8)\n",
        "axes[0, 0].plot(K_list, avg_epi_id_list_de, 's-', label='Epistemic (ID)', color='orange', linewidth=2, markersize=8)\n",
        "axes[0, 0].plot(K_list, avg_tot_id_list_de, '^-', label='Total (ID)', color='blue', linewidth=2, markersize=8)\n",
        "axes[0, 0].set_xlabel('Number of Nets (K)', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Average Uncertainty', fontsize=12)\n",
        "axes[0, 0].set_title(f'Deep Ensemble: Average Uncertainties (ID) vs K\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].set_xticks(K_list)\n",
        "\n",
        "# Plot 2: Average Uncertainties - OOD region\n",
        "axes[0, 1].plot(K_list, avg_ale_ood_list_de, 'o-', label='Aleatoric (OOD)', color='green', linewidth=2, markersize=8)\n",
        "axes[0, 1].plot(K_list, avg_epi_ood_list_de, 's-', label='Epistemic (OOD)', color='orange', linewidth=2, markersize=8)\n",
        "axes[0, 1].plot(K_list, avg_tot_ood_list_de, '^-', label='Total (OOD)', color='blue', linewidth=2, markersize=8)\n",
        "axes[0, 1].set_xlabel('Number of Nets (K)', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Average Uncertainty', fontsize=12)\n",
        "axes[0, 1].set_title(f'Deep Ensemble: Average Uncertainties (OOD) vs K\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].set_xticks(K_list)\n",
        "\n",
        "# Plot 3: MSE comparison\n",
        "axes[1, 0].plot(K_list, mse_id_list_de, 'o-', label='MSE (ID)', color='blue', linewidth=2, markersize=8)\n",
        "axes[1, 0].plot(K_list, mse_ood_list_de, 's-', label='MSE (OOD)', color='red', linewidth=2, markersize=8)\n",
        "axes[1, 0].set_xlabel('Number of Nets (K)', fontsize=12)\n",
        "axes[1, 0].set_ylabel('MSE', fontsize=12)\n",
        "axes[1, 0].set_title(f'Deep Ensemble: MSE vs K\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=10)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_yscale('log')\n",
        "axes[1, 0].set_xticks(K_list)\n",
        "\n",
        "# Plot 4: ID vs OOD comparison (bar chart)\n",
        "x_pos = np.arange(len(K_list))\n",
        "width = 0.35\n",
        "axes[1, 1].bar(x_pos - width/2, avg_epi_id_list_de, width, label='Epistemic (ID)', color='orange', alpha=0.7)\n",
        "axes[1, 1].bar(x_pos + width/2, avg_epi_ood_list_de, width, label='Epistemic (OOD)', color='red', alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Number of Nets (K)', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Average Epistemic Uncertainty', fontsize=12)\n",
        "axes[1, 1].set_title(f'Deep Ensemble: Epistemic Uncertainty - ID vs OOD\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(K_list)\n",
        "axes[1, 1].legend(fontsize=10)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle(f'Deep Ensemble Parameter Comparison: Varying K', \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot\n",
        "save_plot(fig, f\"Deep_Ensemble_K_comparison_{function_name}_{noise_type}\", \n",
        "          subfolder=f\"comparisons/{noise_type}/{func_type}\")\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "# Create summary table\n",
        "comparison_df_de = pd.DataFrame({\n",
        "    'K': K_list,\n",
        "    'Avg_Ale_ID': avg_ale_id_list_de,\n",
        "    'Avg_Epi_ID': avg_epi_id_list_de,\n",
        "    'Avg_Tot_ID': avg_tot_id_list_de,\n",
        "    'Avg_Ale_OOD': avg_ale_ood_list_de,\n",
        "    'Avg_Epi_OOD': avg_epi_ood_list_de,\n",
        "    'Avg_Tot_OOD': avg_tot_ood_list_de,\n",
        "    'MSE_ID': mse_id_list_de,\n",
        "    'MSE_OOD': mse_ood_list_de\n",
        "})\n",
        "\n",
        "print(\"\\nSummary Table:\")\n",
        "print(comparison_df_de.to_string(index=False))\n",
        "\n",
        "# Save comparison table\n",
        "save_statistics(comparison_df_de, f\"Deep_Ensemble_K_comparison_{function_name}_{noise_type}\",\n",
        "                subfolder=f\"comparisons/{noise_type}/{func_type}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall Comparison Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a combined summary\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"OVERALL COMPARISON SUMMARY\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "print(\"MC Dropout - Best Parameters (lowest OOD MSE):\")\n",
        "best_mc_idx = np.argmin(mse_ood_list)\n",
        "best_mc_samples = mc_samples_list[best_mc_idx]\n",
        "print(f\"  MC Samples: {best_mc_samples}\")\n",
        "print(f\"  OOD MSE: {mse_ood_list[best_mc_idx]:.6f}\")\n",
        "print(f\"  OOD Epistemic Uncertainty: {avg_epi_ood_list[best_mc_idx]:.6f}\")\n",
        "\n",
        "print(\"\\nDeep Ensemble - Best Parameters (lowest OOD MSE):\")\n",
        "best_de_idx = np.argmin(mse_ood_list_de)\n",
        "best_K = K_list[best_de_idx]\n",
        "print(f\"  K: {best_K}\")\n",
        "print(f\"  OOD MSE: {mse_ood_list_de[best_de_idx]:.6f}\")\n",
        "print(f\"  OOD Epistemic Uncertainty: {avg_epi_ood_list_de[best_de_idx]:.6f}\")\n",
        "\n",
        "# Create side-by-side comparison plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# MC Dropout: Epistemic uncertainty comparison\n",
        "axes[0].plot(mc_samples_list, avg_epi_id_list, 'o-', label='Epistemic (ID)', color='blue', linewidth=2, markersize=8)\n",
        "axes[0].plot(mc_samples_list, avg_epi_ood_list, 's-', label='Epistemic (OOD)', color='red', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('MC Samples', fontsize=12)\n",
        "axes[0].set_ylabel('Average Epistemic Uncertainty', fontsize=12)\n",
        "axes[0].set_title(f'MC Dropout: Epistemic Uncertainty\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xticks(mc_samples_list)\n",
        "\n",
        "# Deep Ensemble: Epistemic uncertainty comparison\n",
        "axes[1].plot(K_list, avg_epi_id_list_de, 'o-', label='Epistemic (ID)', color='blue', linewidth=2, markersize=8)\n",
        "axes[1].plot(K_list, avg_epi_ood_list_de, 's-', label='Epistemic (OOD)', color='red', linewidth=2, markersize=8)\n",
        "axes[1].set_xlabel('Number of Nets (K)', fontsize=12)\n",
        "axes[1].set_ylabel('Average Epistemic Uncertainty', fontsize=12)\n",
        "axes[1].set_title(f'Deep Ensemble: Epistemic Uncertainty\\n{function_name} Function ({noise_type.capitalize()})', fontsize=13, fontweight='bold')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xticks(K_list)\n",
        "\n",
        "plt.suptitle('Parameter Comparison: ID vs OOD Epistemic Uncertainty', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot\n",
        "save_plot(fig, f\"Overall_comparison_{function_name}_{noise_type}\", \n",
        "          subfolder=f\"comparisons/{noise_type}/{func_type}\")\n",
        "plt.show()\n",
        "plt.close(fig)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
